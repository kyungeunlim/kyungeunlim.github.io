<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A physicist's systematic exploration of transformer architecture and AI interpretability research">
    <meta name="author" content="Kyungeun Lim">

    <title>From Physics to AI Safety: Cross-Disciplinary Questions for Transformer Interpretability</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="static/css/bootstrap.css">

    <style>
      .section-header {
        margin-top: 40px;
        margin-bottom: 20px;
        padding-bottom: 10px;
        border-bottom: 2px solid #337ab7;
        /*border-bottom: 2px solid #2c5282;*/
      }
      .subsection {
        margin-top: 25px;
        margin-bottom: 15px;
      }
      .container ul li {
	  font-size: 16px;
	  line-height: 1.6;
	  margin-bottom: 8px;
      }
      .insight-box {
        background-color: #f8f9fa;
        border-left: 4px solid #337ab7;
        padding: 15px;
        margin: 20px 0;
      }
      .draft-note {
        background-color: #fcf8e3;
        border: 1px solid #faebcc;
        color: #8a6d3b;
        padding: 10px;
        margin-bottom: 20px;
        border-radius: 4px;
      }

      .subsection h3 {
          font-size: 24px;
	  /*color: #337ab7;*/
	  color: #2c5282;
            margin-bottom: 15px;
        }      
     .subsection h4 {
         font-size: 20px;
	 color: #555555;  /* Dark grey */
      }

      .subsection h5 {
            font-size: 18px;
            color: #666666;
            margin-top: 20px;
            margin-bottom: 8px;
      }
      
      .subsection p, .container p {
        font-size: 16px;
        line-height: 1.6;
        margin-bottom: 12px;	
      }
      
      .lead {
        font-size: 22px;
        line-height: 1.5;
      }
        strong {
            color: #2c5282;
        }
        em {
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 3px;
      }

    </style>
</head>

<body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">About</a></li> 
            <li><a href="projects.html">Projects</a></li> 
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

<br><br><br>

    <div class="container">

      <!-- Header -->
      <div class="starter-template">
        <h1>From Physics to AI Safety: Cross-Disciplinary Questions for Transformer Interpretability</h1>
        <p class="lead">A two-week deep dive into transformer architecture: a learning journey from fundamentals to physics-inspired research questions.
	</p>

	
        <div class="draft-note">
          <strong>Mostly complete with missing info in Appendix</strong>
        </div>
      </div>

      <!-- Executive Summary -->
      <div class="section-header">
	<h2>Executive Summary</h2>
      </div>
      <p>This document presents a systematic exploration of transformer architecture through the lens of physics training, building foundational knowledge necessary for mechanistic interpretability research while demonstrating how cross-disciplinary approaches can accelerate understanding of complex AI systems. This work illustrates how domain expertise from physics can provide complementary perspectives on AI safety challenges, with research directions identified that could contribute to advancing interpretability methods, understanding scaling phenomena, and developing more rigorous approaches to analyzing AI systems.</p>
      
      <h3>Key Contributions</h3>
      <ul>
	<li><strong>Physics-Informed Architecture Understanding:</strong> Applied physics frameworks (perturbation theory, many-body systems, statistical mechanics) to understand transformer mechanics, revealing analogies that clarify complex mechanisms like residual streams and attention patterns</li>
  
	<li><strong>Systematic Learning Methodology:</strong> Developed and documented a transferable 6-step process for efficiently acquiring expertise across technical domains—valuable for researchers transitioning into AI safety from other fields</li>
  
	<li><strong>Physics-Inspired Research Directions:</strong> Identified potential research avenues including group theory applications to neural circuit classification, scaling laws as phase transitions, and statistical mechanics approaches to emergent behavior—questions that may offer fresh methodological angles for interpretability research</li>
  
	<li><strong>Technical Implementation:</strong> Systematically worked through transformer implementations with hands-on exploration of attention mechanisms, testing different inputs and modifications to build deep understanding of architectural components.</li>
      </ul>
      
      <!-- Section 1: Motivation -->
      <div class="section-header">
        <h2>1. Why: Motivation for This Deep Dive</h2>
      </div>

      <div class="subsection">
	<h3>The Pull: Why AI Interpretability Matters</h3>
	<!--<p class="lead">-->
	<p> I've always enjoyed learning broadly and deeply—from podcasts and audiobooks spanning big history, brain science, and philosophy to technical tutorials and online courses—driven mostly by curiosity and genuine enjoyment. This habit of connecting insights across seemingly unrelated domains has shaped how I approach complex problems: looking for patterns that emerge when viewing challenges through multiple disciplinary lenses. But this goes beyond personal curiosity—I also believe learning isn't optional in this era of exploding knowledge; it's directly related to our collective survival. The COVID pandemic illustrated this perfectly: without mRNA vaccine technology—built on decades of accumulated scientific knowledge across multiple fields—humanity could have faced an existential threat.</p>

	<p>Yet this necessity faces a daunting reality: The rapidly exploding pace of new knowledge made me realize I need to be more efficient at learning itself—I've been pondering what would be the most effective way of learning how to learn. Additionally, recently experiencing the loss of loved ones and witnessing devastating events so close to home reinforced that time is our scarcest resource. </p>

	  <p>These convergent realizations created a dilemma: I can learn forever and find joy in it, but what's the point if I don't use it for something meaningful? This led me to reflect deeply on what problems I feel strongly enough about to be worth my limited time, and how to balance leveraging my existing knowledge with efficiently adopting new tools to tackle challenges that matter.</p>

	<p>The answer traces back to questions that have fascinated me since watching "Ghost in the Shell" in the 90s: What makes a human human? My early conclusion was "memory"—and after learning more about brain science, I'm convinced this intuition was correct. Memory is fundamental to emotion, action, identity, and consciousness. Recent films like "After Yang" have renewed these questions about the boundary between human and AI.</p>

	<!--
	<p>This balance became clear when I considered AI interpretability research. The field can benefit from rigorous analytical skills to reverse-engineer complex systems—drawing from physics training—and practical experience bridging theory with real-world applications. Interpretability sits at this intersection, combining both scientific rigor and engineering pragmatism while rapidly evolving with new techniques and tools.</p>-->

	<p>With AI systems becoming increasingly sophisticated, I see interpretability research as the perfect intersection of my physics training, my industry experience, and my long-standing fascination with consciousness. Understanding how these systems actually work—reverse-engineering their "memory" and decision-making—feels like the most meaningful problem I can contribute to. I also believe that understanding current AI is a prerequisite for safely developing artificial superintelligence (ASI). It combines the rigor of physics with the profound questions about intelligence that have captivated me for decades, while addressing what I see as a critical foundation for humanity's AI future.
	</p>
      </div>

      <div class="subsection">
        <h3>This Project: Learning Experiment</h3>
	<p>This project also served as an experiment in applying recent insights from brain science to my learning approach. I'd learned that our brains work differently from muscles—the key isn't avoiding fatigue but leveraging how creativity emerges from the default mode network through novel combinations of existing knowledge. I wanted to challenge my traditional "deep focus for a few hours" approach, which I considered my core strength and was often praised for, and become more flexible with planning, revising scope on the fly rather than spending days perfecting initial plans.</p>
	</div>      

      
      <div class="subsection">
        <h3>Planning: Two-Week Timeline & Scope</h3>
	
	<p>With upcoming travel scheduled in four weeks, I saw an opportunity to test whether interpretability research was the right direction for my career transition. Drawing from my previous ultra learning experience, I allocated two weeks for deep exploration, one week for writing, and kept the final week as contingency—a more realistic timeline than my perfectionist tendencies usually allow.</p>

	<p>The biggest challenge was that the scope would entirely depend on how efficiently I could learn the fundamentals. Based on early conversations with friends familiar with the field, I pivoted from attempting meaningful analysis using existing methods to focusing on building a solid foundational understanding. I reduced the scope to what I could learn effectively and to the unique perspectives or questions I might contribute through a physicist’s lens. This means I intentionally left out existing interpretability research, which I plan to revisit in future work.</p>


      </div>
      <!--
      <div class="subsection">
        <h3>Anthropic's Interpretability Agenda</h3>
        <p>[Fill in: What drew me to their specific approach - mechanistic interpretability, production model focus, circuit analysis methods, I might skip]</p>
      </div> -->

      <!-- Section 2: Learning Methodology -->
      <div class="section-header">
        <h2>2. How: My Learning Methodology</h2>
      </div>

      <div class="subsection">
        <h3>Prior Knowledge Baseline</h3>
        
        <h4>Neural Networks Foundation</h4>
        <p>I already had basic knowledge of neural networks from my academic time - at the very core, they are inspired by our brain, and their power comes from "activation" functions of neurons, which introduce non-linearity and allow the network to learn complex patterns rather than just linear combinations. I also knew that "deep" means the number of layers is larger, which allows better pattern recognition.</p>

        <h4>Sequential Models Intuition</h4>
	<figure class="centered_image">
	  <center>
	    <img src="static/transformer/sequential_models_evolution.png" width="1000px" >
	  </center>
	  <figcaption>Fig.1: Evolution of Sequential Pattern Understanding: From simple Markov chains to sophisticated attention mechanisms, showing how each approach addresses limitations of previous methods.</figcaption>
	</figure>
	<br>
        <p>With my physicist mindset, I had this picture in mind: progressively building up the dimensions of information from previous steps. Markov chains utilize one previous step to make predictions, Fibonacci-style problems utilize two previous steps, and time series models utilize multiple previous steps with exponentially decaying weights, focusing more on recent information. RNNs represent the next evolution where we can theoretically use all previous information in a sequence, but they suffer from vanishing gradients over long distances. LSTMs solved this through sophisticated gating mechanisms that can selectively remember and forget information across long sequences. Attention mechanisms represent the latest breakthrough where I don't have to process information linearly—I can intelligently focus on the most relevant parts of the entire sequence simultaneously, making optimal use of multiple previous steps. All of these advances were only possible with the development of powerful computers, allowing many complex operations within much shorter times (similar to how Bayesian inference became practical due to better computing power). Like almost any technological evolution, this represents repeated attempts to solve the same fundamental problem—understanding sequential patterns—by addressing the limitations of previous methodologies with advancing computational capabilities.</p>

        <h4>Previous Transformer Exposure</h4>
        <p>I also skimmed through the "Attention is All You Need" paper years ago and understood that it essentially removed the need for RNNs in many sequence modeling tasks. More recently, I also took Andrew Ng's Coursera courses to have a better understanding of both neural networks and deep learning fundamentals as well as generative AI with language models. These were also to gain hands-on implementation experience and build basic knowledge of the necessary tools.</p>

        <h4>Practical Experience</h4>
	<p>I led the development of P-Rex, a personalized recommendation system using XGBoost at CloudTrucks, which gave me hands-on experience with feature engineering and production ML deployment. Later, I built an LSTM-based forecasting model, providing exposure to PyTorch and neural network implementation. My work with PostgreSQL and recommendation systems for search ranking also familiarized me with query-key-value concepts used for attention mechanisms in Transformers, though I hadn't worked directly with Transformer architectures previously.</p>

	
      </div>

      <div class="subsection">
        <h3>Learning Strategy: Fundamentals Over Trends</h3>
	<p>Feeling a strong urgency to understand how AI systems actually work, I discovered Anthropic's Mechanical Interpretability Research Scientist job posting and was immediately drawn to their approach of reverse-engineering neural networks. I wanted to quickly assess whether this was the research direction I should pursue, so I dove into the extensive resources provided in their job posting guidelines. </p>

	<p> However, after initially trying to follow all available sources sequentially, I became overwhelmed. I started pondering whether I needed to explore the most recent research to stay "current" or return to fully understanding fundamentals. Stepping back, I realized that if my ultimate goal was to understand how AI works through reverse-engineering transformers, it made no sense to skip understanding transformers from scratch. This reminded me of spending considerable time on harmonic oscillators in Classical Mechanics and hydrogen atoms in Quantum Mechanics—fundamentals are essential building blocks.</p>

	<p>Building on that, it's simply impossible to "stay current" in depth without solid basics, and this approach aligned with my physics background and natural preference for understanding foundational principles first. I made a conscious decision to prioritize foundational understanding over keeping up with the latest papers, viewing this as both a reality check on my capacity and a strategic choice that matched my learning style.</p>

	<p>Finally, I wanted to preserve my "novice" perspective—those fresh questions and potential challenges to existing assumptions that are valuable precisely because of their outsider viewpoint. I knew this perspective would inevitably disappear as I became more familiar with the field, so I made sure to log these initial insights while they were still genuine. Beginner's perspectives can sometimes reveal overlooked assumptions or alternative approaches that experts might miss, but only if captured before that fresh lens is lost to familiarity.</p>
	


      <div class="subsection">
        <h3>Systematic Learning Approach</h3>
	<p>My approach to learning something fundamentally new—especially at the cutting edge—relies on structuring the experience to both accelerate early understanding and build deep intuition over time. I’ve refined this process through years of transitioning between disciplines, and for this deep dive, I leaned heavily on strategies rooted in physics training, modular thinking, brain science insights, and industry experience with rapid iteration cycles. I've distilled this into a repeatable six-step process:</p>

	<h4>1. Learn terminology & connect to existing knowledge</h4>
	<p>Memorize new concepts with clear definitions to create basic building blocks—I then only need to figure out hierarchical relationships among them. This follows the Rumpelstiltskin principle: you can't truly understand what you can't name. I also noticed that as I accumulate knowledge across fields, I increasingly spot similar concepts used in different domains.</p>

	<h4>2. Map hierarchical structure</h4>
	<p>In books or papers, this is equivalent to understanding the table of contents. This is where I personally benefit most from LLMs. When deciding whether to read a paper, I ask: 1) summarize the whole paper in a few bullet points, 2) what are the strengths and weaknesses, and 3) how would you improve it? I learned this framework from <a href="https://ko.wikipedia.org/wiki/박태웅_(1963년)" target="_blank">Tae-woong Park</a>'s YouTube videos and initially applied it by manually prompting various LLMs. However, as AI tools evolve rapidly, I've since streamlined this process—I now delegate these questions directly to <a href="https://notebooklm.google.com/" target="_blank">NotebookLM</a>, which has become my default tool for this type of analysis.</p>
	
	<h4>3. Skim fast, focus on gaps</h4>
	<p>This is based on the fact that I already have accumulated fundamental knowledge from other work (basic math, science concepts, etc.). To be efficient about learning new things, I can skim through everything at high speed, then focus on parts I don't understand, get stuck on, or find insightful.</p>

	<h4>4. Test with hands-on coding</h4>
	<p>This is hypothesis testing—understanding output with different inputs builds real understanding and shows how I can materialize/execute ideas.</p>

	<h4>5. Create visual summaries</h4>
	<p>From my academic experience, I learned that eventually any research result is summarized as either a figure or a table. Diagrams are visual aids I often use to communicate complicated concepts with team members when presenting projects.</p>

	<h4>6. Test through active recall</h4>
	<p>The real test is whether I can explain each concept clearly—this is the critical part of turning memory into knowledge. Writing is often a good way to do this, and this blog also serves this purpose.</p>

	<h4> Example: This work</h4>
	<p>A detailed walkthrough of applying this methodology is provided in the <a href="#learning-process-example">Appendix</a>.</p>	
	
      <!-- Section 3: Current Understanding & Physics Parallels -->
      <div class="section-header">
        <h2>3. What: A Physicist’s Understanding of Transformers</h2>
      </div>

      <div class="subsection">
        <h3>Transformer Architecture Deep Dive</h3>
	  <p>In this section, we focus on a decoder-only transformer, specifically using the same architecture and parameters as GPT-2 Small <a href="#ref-radford2019" class="citation">[1]</a>.</p>


	<figure class="centered_image">
	  <center>
		<img src="static/transformer/transformer_flowchart_300.drawio.png" width="300px">
	      </center>
	     <figcaption style="text-align: center;">
	    Fig.2: Flowchart of the Decoder-Only Transformer. It is slightly modified from the figure shown <a href="https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/Clean_Transformer_Demo_Template.ipynb">here</a>.
	  </figcaption>
	</figure>
	<br>


	<h4>Foundational Concepts</h4>
        
        <h5>The Residual Stream: Information Highway</h5>
        <p>At the heart of the transformer lies the <strong>residual stream</strong> - the main information highway that carries data through each layer. Unlike traditional neural networks where information gets completely transformed at each step, transformers add modifications to this stream while preserving the original information. This is analogous to how we might annotate a document by adding notes in the margins rather than rewriting the entire text. From a physics perspective, this is similar to perturbation theory - we start with an unperturbed state (the original embeddings) and add small corrections at each layer, with each transformer block acting like a perturbative correction that modifies but doesn't replace the underlying state.</p>

	<h5>Layer Normalization: Preventing Gradient Problems</h5>
	<p>Layer normalization's primary purpose is maintaining stable gradient flow during training. In deep networks, activations can have wildly different scales across layers, causing gradients during backpropagation to become very small (vanishing) or very large (exploding). Layer normalization solves this by normalizing the activations going into each layer, keeping their scale consistent.</p>

	<p>The process: subtract the mean to center at zero, divide by standard deviation to normalize variance, then apply learned scaling and translation parameters. This ensures more stable gradient magnitudes throughout the network during training, allowing the model to learn effectively. The learned scaling (γ) and translation (β) parameters then allow each layer to recover the optimal scale and shift for its specific function, while maintaining the gradient flow benefits.</p>

	<p>This creates more stable optimization dynamics, similar to how physicists normalize variables in numerical simulations to prevent computational instabilities when dealing with vastly different scales.</p>
	

	<h4>The Transformer Flow</h4>
	See Fig.2 above starting from bottom to top.

	<h5>1. Tokens: From Text to Integers</h5>
        <p>Everything starts with converting human-readable text into integers through tokenization. This uses a method called Byte-Pair Encoding - we start with individual characters and iteratively merge the most common pairs. The result is a vocabulary of ~50,000 sub-word tokens that can represent any text. Each token gets mapped to a unique integer ID, which serves as an efficient key for the embedding lookup table.</p>

        <h5>2. Embedding: From Integers to Vectors</h5>
        <p>The embedding layer is essentially a giant lookup table that converts each token ID into a high-dimensional vector (768 dimensions in GPT-2). This learned mapping allows the model to represent semantic relationships - similar tokens end up with similar embedding vectors through training.</p>	


        <h5>3. Positional Embeddings: Adding Sequence Information</h5>
        <p>Since attention is content-based rather than position-based, we need to explicitly tell the model where each token sits in the sequence. We learn another lookup table that maps position indices to vectors, then <em>add</em> these to the token embeddings. The key insight that initially confused me: how does adding positional embeddings to token embeddings not create collisions? The answer lies in the high dimensionality - with embedding vectors of length 768, the probability of collision becomes vanishingly small. This is similar to how CRISPR achieved precision in gene editing: while earlier techniques used shorter recognition sequences (higher collision risk), CRISPR uses ~20-nucleotide guide sequences, making the probability of accidentally matching the wrong genomic location negligibly small. </p>

        <h5>4. Attention Mechanism: Intelligent Information Routing</h5>
        <p>This is where the magic happens. For each token, the model computes three vectors: Query (what am I looking for?), Key (what do I contain?), and Value (what information should I pass along?). The similarity between Query and Key vectors determines how much attention each token pays to every other token in the sequence. (During implementation, I discovered that Einstein summation order doesn't affect the results—a satisfying confirmation that dot product commutativity holds even in these complex operations.)</p>
	<p>**Note: In decoder-only transformers like GPT-2, this is specifically self-attention - each token attends to other tokens within the same sequence. This differs from encoder-decoder architectures (like the original Transformer) which also include cross-attention, where decoder tokens attend to encoder representations. Since we have no separate encoder, all attention is self-attention.**</p>
	
        <p><strong>Multi-Head Attention:</strong> Rather than having one attention mechanism, transformers use multiple "heads" (12 in GPT-2) that attend to different types of information simultaneously. Each head can specialize - one might focus on syntactic relationships, another on semantic meaning. It's like having multiple experts each contributing their specialized perspective.</p>
        

<p><strong>Causal Attention:</strong> In GPT-style models, we use what's called "causal attention" - though I find this term somewhat misleading from a physicist's perspective. While there's a notion of sequence, there's no true temporal causality. Instead, it's a masking mechanism that prevents tokens from <em>seeing</em> later tokens in sequence, maintaining the sequential prediction task. This is analogous to preventing data leakage in ML (where future information accidentally gets into training features) or blinding in experimental physics (where researchers are kept unaware of group assignments to prevent bias). All three techniques share the same core principle: controlling information flow to maintain the integrity of the process.</p>

        <h5>5. MLP Layer: Pattern Recognition and Computation</h5>
	<p>Despite the confusing name "multi-layer perceptron," these are actually simple two-layer feed-forward networks: expand the dimensionality (768 → 3072), apply non-linearity (GELU activation), then contract back (3072 → 768). The expansion (768 → 3072) creates finer granularity by providing more "working space" for complex transformations. The GELU non-linearity then selectively activates different pathways, creating pattern recognition. The contraction (3072 → 768) distills all this rich intermediate computation back into the residual stream format, essentially summarizing the complex high-dimensional processing into a form that can be added back to the information highway. </p>


	<h5>6. Transformer Blocks: The Core Architecture</h5>
	<p>The transformer block is the fundamental building unit, consisting of two key steps that always work together in a specific order:</p>

	<p><strong>1. Attention Mechanism:</strong> Routes and gathers relevant information from across the sequence (Communication Phase) <br>
	  <strong>2. MLP Layer:</strong> Processes and transforms this contextualized information (Computation Phase)</p>

	<p>This order is crucial: attention first allows each position to collect relevant context from the entire sequence, then the MLP can do informed processing with this enriched representation. The alternative (MLP → Attention) would force the MLP to process information blindly at each position before sharing, like trying to analyze the word "bank" without knowing whether the context involves rivers or money. By putting attention first, we follow the principle of "gather information, then process it" rather than "process locally, then share." This makes computation more effective - the MLP gets to do its complex reasoning on contextualized, relevant information rather than wasting computational effort processing isolated local information that may lack important context. </p>

	<p>Each step is preceded by layer normalization (a standard preprocessing necessity before feeding information to any layer, including the final unembed step). Both the attention and MLP outputs are added back to the residual stream via residual connections.</p>

	<p>This transformer block pattern repeats 12 times in GPT-2, with each iteration building increasingly complex representations from the combination of attention-driven information gathering and MLP-driven computation.</p>


	<h5>7. Unembed: Converting to Vocabulary Scores</h5>
	<p>The final step converts the high-dimensional representations into raw scores over the vocabulary. This is conceptually the reverse of embedding - we apply a linear transformation (matrix multiplication plus bias) from the model dimension (768) to vocabulary size (~50,000), creating one logit score for each possible token type in the vocabulary.</p>


	<h5>8. Logits: Raw Predictions</h5>
	<p>These raw scores (shown at the top of Fig. 2) represent the model's unprocessed predictions. During inference, they are converted to probabilities using softmax (not shown in Fig. 2 as it's a mathematical operation, not an architectural component), giving us probability estimates for each possible next token. For token generation, we can sample from this distribution or take the highest probability token. These probabilities also serve as the primary metric for measuring prediction quality, used for cross-entropy loss during both training and evaluation, perplexity calculations, and other language modeling metrics.</p>	

      </div>

      
      <!-- Section 4: Research Questions -->
      <div class="section-header">
        <h2>4. So What?: Physics-Inspired Open Questions</h2>
      </div>
      <p>The following research directions reflect physics-informed perspectives and open questions that I have developed during my exploration; while they may overlap with prior work, I believe they highlight useful angles for further investigation.</p>
      
      <div class="subsection">
        <h3>Potential Research Directions</h3>

	<h4>Physics-Inspired Frameworks</h4>
	<p>Could we apply group theory (successfully used to organize the "particle zoo" in the 1950s-60s) to systematically group neural circuit patterns for better interpretability? Just as group theory revealed underlying symmetries that organized seemingly random hadrons into the Eightfold Way and eventually led to the quark model, similar mathematical frameworks might help classify the multitude of neural network pathways and patterns.</p>

	<h4>Many-Body Neural Systems</h4>
	<p>While we understand individual neurons, understanding group behavior remains elusive - reminiscent of the reductionist vs. holistic challenge in physics. Statistical mechanics deals with many-body systems and emergent laws. Could these approaches better explain neural network behavior than trying to understand individual components?</p>
	
	<h4>Scaling Laws as Phase Transitions</h4>
	<p>Do scaling laws in neural networks correspond to phase transitions observed in physics? Certain phenomena require critical mass (like star formation) - what are the analogous thresholds in neural systems where large numbers create qualitatively different behavior?</p>
      </div>

      <div class="subsection">
        <h3>Methodological Questions</h3>
        <!--<p>[Fill in: Questions about experimental design, measurement, analysis approaches that could benefit from physics thinking]</p>-->
	<h4>Measurement Strategy</h4>
	<p>From an experimental physicist's perspective, <em>what</em> we measure is critical. Is loss really the best metric, or should we focus on token probabilities or layer weights? How do we systematically measure emergent behavior with scaling? With so many variables to experiment on, how can we quickly and systematically group or reduce them?</p>

	<h4>Uncertainty vs. Deterministic Classification</h4>
	<p>Physics emphasizes precise measurements with quantified uncertainties, while industry ML often defaults to deterministic outputs. Given that the human brain operates as a Bayesian inference engine—continuously updating beliefs from limited evidence—should interpretability research embrace more explicitly Bayesian frameworks? This could be particularly valuable for understanding how transformers handle infrequent but critical decisions where we can't gather sufficient statistics, similar to how humans make consequential life choices without the luxury of controlled trials.</p>

	<h4>Brain Science vs. Mathematical Approaches</h4>
	<p>Given that neural networks aim to emulate brains, shouldn't we leverage brain science approaches more heavily? (More detailed explantion need to be added)</p>
      </div>

      <div class="subsection">
        <h3>Fundamental Puzzles</h3>
	<h4>Grounded vs. Abstract Meaning</h4>
	<p>Neural networks feel more mathematical than physical - in physics, information has specific meaning (e.g., particle spin states in wave functions correspond to measurable angular momentum), but in ML, any numerical representation is valid as long as it's useful for the task. Why should certain neuron firing patterns have inherent meaning? This fundamental question about the nature of representation needs addressing.</p>


	<h4>In-Context Learning</h4>
	<p>How does transformer in-context learning compare to human rapid adaptation? The ability to change behavior without parameter updates seems more like pattern recognition than true learning—what can neuroscience tell us about this distinction?</p>
      </div>

      <!-- Section 5: Reflection & Next Steps 
      <div class="section-header">
        <h2>5. Reflection: What This Process Taught Me</h2>
      </div>

      <div class="subsection">
        <h3>Key Insights</h3>
        <p>[Fill in: Most important things you learned about the field and about your own approach to learning]</p>
      </div>

      <div class="subsection">
        <h3>Confidence in Career Direction</h3>
        <p>[Fill in: How this experience reinforced your decision to pursue interpretability research at Anthropic, need to ponder about it]</p>
      </div>

      <div class="subsection">
        <h3>Next Steps</h3>
        <p>[Fill in: Specific areas you want to explore further, skills to develop, research directions to pursue]</p>
      </div>
      -->

      <!-- Appendix -->
      <div class="section-header">
	<h2>Appendix</h2>
	<div class="insight-box">
	  <p>[<em>As of 2025-08-17, this section is incomplete. Needs to be updated.</em>]</p>
	</div>
    
	<!-- Add References as first subsection -->
	<h3>References</h3>
	<ol>
	  <li id="ref-radford2019">
            Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). 
            <em>Language Models are Unsupervised Multitask Learners</em>. OpenAI. 
            <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">PDF</a>
	  </li>
	  <!-- Add other references as needed -->
	</ol>
    
	<h3>Tools and Resources</h3>
	<h3 id="learning-process-example">Example: Applying My Six-Step Learning Process</h3>

	<!-- Footer -->
	<div class="starter-template">
	  <hr>
	  <p class="text-center">
	    <a href="projects.html" class="btn btn-default">← Back to Projects</a>
	  </p>
	  <p class="text-center">
	    Questions about this research? Find me on
	    <a href="https://www.linkedin.com/in/kyungeun-lim" target="_blank">LinkedIn</a> |
	    <a href="https://github.com/kyungeunlim" target="_blank">GitHub</a>
	  </p>
	</div>
      
    </div><!-- /.container -->

    <!-- Bootstrap core JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="static/js/bootstrap.min.js"></script>

  </body>
</html>
