<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A physicist's systematic exploration of transformer architecture and AI interpretability research">
    <meta name="author" content="Kyungeun Lim">

    <title>Transformer Architecture Deep Dive: A Physics Perspective</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="static/css/bootstrap.css">

    <style>
      .section-header {
        margin-top: 40px;
        margin-bottom: 20px;
        padding-bottom: 10px;
        border-bottom: 2px solid #337ab7;
        /*border-bottom: 2px solid #2c5282;*/
      }
      .subsection {
        margin-top: 25px;
        margin-bottom: 15px;
      }
      .insight-box {
        background-color: #f8f9fa;
        border-left: 4px solid #337ab7;
        padding: 15px;
        margin: 20px 0;
      }
      .draft-note {
        background-color: #fcf8e3;
        border: 1px solid #faebcc;
        color: #8a6d3b;
        padding: 10px;
        margin-bottom: 20px;
        border-radius: 4px;
      }

      .subsection h3 {
          font-size: 24px;
	  /*color: #337ab7;*/
	  color: #2c5282;
            margin-bottom: 15px;
        }      
     .subsection h4 {
         font-size: 20px;
	 color: #555555;  /* Dark grey */
      }

      .subsection h5 {
            font-size: 18px;
            color: #666666;
            margin-top: 20px;
            margin-bottom: 8px;
        }
      .subsection p, .container p {
        font-size: 16px;
        line-height: 1.6;
        margin-bottom: 12px;	
      }
      
      .lead {
        font-size: 22px;
        line-height: 1.5;
      }
        strong {
            color: #2c5282;
        }
        em {
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 3px;
        }	
  </style>
</head>

<body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">About</a></li> 
            <li><a href="projects.html">Projects</a></li> 
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

<br><br><br>

    <div class="container">

      <!-- Header -->
      <div class="starter-template">
        <h1>Transformer Architecture Deep Dive</h1>
        <p class="lead">A Physics Perspective on Understanding Transformer Mechanisms
	</p>
        
        <div class="draft-note">
          <strong>Work in Progress:</strong> This is a documentation of my 2-week deep dive into transformer architecture and interpretability research, written as part of my transition to AI safety research.
        </div>
      </div>

      <!-- Executive Summary - To be written last -->
      <div class="section-header">
        <h2>Executive Summary</h2>
      </div>
      <div class="insight-box">
        <p><em>[To be filled after completing the full writeup - will summarize key insights, physics connections discovered, and research questions formulated]</em></p>
      </div>

      <!-- Section 1: Motivation -->
      <div class="section-header">
        <h2>1. Motivation: Why This Deep Dive?</h2>
      </div>

      <div class="subsection">
	<h3>Focus: Why AI Interpretability</h3>
	<!--<p class="lead">-->
	  <p>I've always enjoyed learning broadly and deeply—from podcasts and audiobooks on diverse topics to technical tutorials and online courses—driven mostly by curiosity and genuine enjoyment. But I also believe learning isn't optional in this era of exploding knowledge; it's directly related to our collective survival. The COVID pandemic illustrated this perfectly: without mRNA vaccine technology—built on decades of accumulated scientific knowledge—humanity could have faced an existential threat. </p>

	  <p>The rapidly exploding pace of new knowledge made me realize I need to be more efficient at learning itself: I've been pondering what would be the most effective way of learning how to learn. In addition to it, recently experiencing the loss of loved ones and witnessing devastating events so close to home reinforced that time is our scarcest resource. </p>

	  <p>These convergent realizations created a dilemma: I can learn forever and find joy in it, but what's the point if I don't use it for something meaningful? This led me to reflect deeply on what problems I feel strongly enough about to be worth my limited time, and how to balance leveraging my existing knowledge with efficiently adopting new tools to tackle challenges that matter.</p>

	<p>The answer traces back to questions that have fascinated me since watching "Ghost in the Shell" in the 90s: What makes a human human? My early conclusion was "memory"—and after learning more about brain science, I'm convinced this intuition was correct. Memory is fundamental to emotion, action, identity, and consciousness. Recent films like "After Yang" have renewed these questions about the boundary between human and AI.</p>

	<!--
	<p>This balance became clear when I considered AI interpretability research. The field can benefit from rigorous analytical skills to reverse-engineer complex systems—drawing from physics training—and practical experience bridging theory with real-world applications. Interpretability sits at this intersection, combining both scientific rigor and engineering pragmatism while rapidly evolving with new techniques and tools.</p>-->

	<p>With AI systems becoming increasingly sophisticated, I see interpretability research as the perfect intersection of my physics training, my industry experience, and my long-standing fascination with consciousness. Understanding how these systems actually work—reverse-engineering their "memory" and decision-making—feels like the most meaningful problem I can contribute to. I also believe that understanding current AI is a prerequisite for safely developing artificial superintelligence (ASI). It combines the rigor of physics with the profound questions about intelligence that have captivated me for decades, while addressing what I see as a critical foundation for humanity's AI future.
	</p>
      </div>

      <div class="subsection">
        <h3>Approach: Experimental Learning & Communication</h3>

	<p>This project also served as an experiment in applying recent insights from brain science to my learning approach. I'd learned that our brains work differently from muscles—the key isn't avoiding fatigue but leveraging how creativity emerges from the default mode network through novel combinations of existing knowledge. I wanted to challenge my traditional "deep focus for a few hours" approach, which I considered my core strength and was often praised for, and become more flexible with planning, revising scope on the fly rather than spending days perfecting initial plans.</p>

	<p>[fill-in: Need to write why i thought writing this write up was important for communcation] </p?
	
	</div>      

      
      <div class="subsection">
        <h3>Planning: Two-Week Timeline</h3>
        <p>[Fill in: Why I chose interpretability research, discovering Anthropic's work, deciding to go deep rather than surface-level learning]</p>
	
	<p>With upcoming travel scheduled in four weeks, I saw an opportunity to test whether interpretability research was the right direction for my career transition. Drawing from my previous ultra learning experience, I allocated two weeks for deep exploration, one week for writing, and kept the final week as contingency—a more realistic timeline than my perfectionist tendencies usually allow.</p>

      <p>The biggest challenge was that scope would entirely depend on how efficiently I could learn the fundamentals. Based on early conversations with friends familiar with the field, I pivoted from attempting meaningful analysis using existing methods to focusing on building solid foundational understanding. I reduced scope to what I could learn effectively and what unique perspectives or questions I might contribute through a physicist's lens.</p>

      </div>
      
      <div class="subsection">
        <h3>Anthropic's Interpretability Agenda</h3>
        <p>[Fill in: What drew me to their specific approach - mechanistic interpretability, production model focus, circuit analysis methods, I might skip]</p>
      </div>

      <!-- Section 2: Learning Methodology -->
      <div class="section-header">
        <h2>2. Learning Methodology</h2>
      </div>

      <div class="subsection">
        <h3>Prior Knowledge Baseline</h3>
        
        <h4>Neural Networks Foundation</h4>
        <p>I already had basic knowledge of neural networks from my academic time - at the very core, they are inspired by our brain, and their power comes from "activation" functions of neurons, which introduce non-linearity and allow the network to learn complex patterns rather than just linear combinations. I also knew that "deep" means the number of layers is larger, which allows better pattern recognition.</p>

        <h4>Sequential Models Intuition</h4>
	<figure class="centered_image">
	  <center>
	    <img src="static/transformer/sequential_models_evolution.png" width="1000px" >
	  </center>
	  <figcaption>Fig.1: Evolution of Sequential Pattern Understanding: From simple Markov chains to sophisticated attention mechanisms, showing how each approach addresses limitations of previous methods.</figcaption>
	</figure>
	<br>
        <p>With my physicist mindset, I had this picture in mind: progressively building up the dimensions of information from previous steps. Markov chains utilize one previous step to make predictions, Fibonacci-style problems utilize two previous steps, and time series models utilize multiple previous steps with exponentially decaying weights, focusing more on recent information. RNNs represent the next evolution where we can theoretically use all previous information in a sequence, but they suffer from vanishing gradients over long distances. LSTMs solved this through sophisticated gating mechanisms that can selectively remember and forget information across long sequences. Attention mechanisms represent the latest breakthrough where I don't have to process information linearly—I can intelligently focus on the most relevant parts of the entire sequence simultaneously, making optimal use of multiple previous steps. All of these advances were only possible with the development of powerful computers, allowing many complex operations within much shorter times (similar to how Bayesian inference became practical due to better computing power). Like almost any technological evolution, this represents repeated attempts to solve the same fundamental problem—understanding sequential patterns—by addressing the limitations of previous methodologies with advancing computational capabilities.</p>

        <h4>Previous Transformer Exposure</h4>
        <p>I also skimmed through the "Attention is All You Need" paper years ago and understood that it essentially removed the need for RNNs in many sequence modeling tasks. More recently, I also took Andrew Ng's Coursera courses to have better understanding of both neural networks and deep learning fundamentals as well as generative AI with language models. These were also to gain hands-on implementation experience and build basic knowledge of the necessary tools.</p>

        <h4>Practical Experience</h4>
	<p>I led the development of P-Rex, a personalized recommendation system using XGBoost at CloudTrucks, which gave me hands-on experience with feature engineering and production ML deployment. Later, I built an LSTM-based forecasting model, providing exposure to PyTorch and neural network implementation. My work with PostgreSQL and recommendation systems for search ranking also familiarized me with query-key-value concepts used for attention mechanisms in Transformers, though I hadn't worked directly with Transformer architectures previously.</p>

	
      </div>

      <div class="subsection">
        <h3>Learning Strategy: Fundamentals Over Trends</h3>
	<p>Feeling a strong urgency to understand how AI systems actually work, I discovered Anthropic's Mechanical Interpretability Research Scientist job posting and was immediately drawn to their approach of reverse-engineering neural networks. I wanted to quickly assess whether this was the research direction I should pursue, so I dove into the extensive resources provided in their job posting guidelines. </p>

	<p> However, after initially trying to follow all available sources sequentially, I became overwhelmed. I started pondering whether I needed to explore the most recent research to stay "current" or return to fully understanding fundamentals. Stepping back, I realized that if my ultimate goal was to understand how AI works through reverse-engineering transformers, it made no sense to skip understanding transformers from scratch. This reminded me of spending considerable time on harmonic oscillators in Classical Mechanics and hydrogen atoms in Quantum Mechanics—fundamentals are essential building blocks.</p>

	<p>It's simply impossible to "stay current" in depth without solid basics, and this approach aligned with my physics background and natural preference for understanding foundational principles first. I made a conscious decision to prioritize foundational understanding over keeping up with the latest papers, viewing this as both a reality check on my capacity and a strategic choice that matched my learning style.</p>

	<p>Additionally, I wanted to preserve my "novice" perspective—those fresh questions and potential challenges to existing assumptions that are valuable precisely because of their outsider viewpoint. I knew this perspective would inevitably disappear as I became more familiar with the field, so I made sure to log these initial insights while they were still genuine. Beginner's perspectives can sometimes reveal overlooked assumptions or alternative approaches that experts might miss, but only if captured before that fresh lens is lost to familiarity.</p>
	


      <div class="subsection">
        <h3>Learning Philosophy and Execution</h3>
	<p>My approach to learning something fundamentally new—especially at the cutting edge—relies on structuring the experience to both accelerate early understanding and build deep intuition over time. I’ve refined this process through years of transitioning between disciplines, and for this deep dive, I leaned heavily on strategies rooted in physics training, modular thinking, and cognitive neuroscience insights.</p>

	<h4>Modularization, Sequentialization, Symmetrization</h4>
	<p> [Update with the notes on yt clips]</p>
	<h4>Building from Familiar Foundations</h4>
	<p>[Need to ponder further for this section</p>

	<h4>Speed + Repetition: Leveraging Audio and Fast Media </h4>
	<p>Speed and repetition are key. For passive intake (lectures, tutorials, interviews), I always listened at 2x speed, pausing frequently to write down ideas that felt intuitively important. For instance, I’d slow down when Andrej Karpathy mentioned early versions of attention as “RNN soft search,” because... [I need to rewrite this part] it clicked with earlier architecture trade-offs I was studying.</p>

	<h4>Execution: From Resources to Real Understanding</h4>
	<p>The learning pipeline I followed looked roughly like this:</p>
	<ol>
	  <li><strong>Curate Resources:</strong> I started with core videos, blog posts, and milestone papers (TransformerLens docs, Neel Nanda’s tutorials, Karpathy’s lectures).</li>
  
	  <li><strong>Translate for Absorption:</strong> Used NotebookLM to parse dense material and draft my own study guides with glossaries, quizzes, and narrative summaries.</li>
  
	  <li><strong>Internalize Concepts:</strong> (Need to rewrite this) Repeated exposure—glossary → quiz → briefing doc → audio overview—allowed me to test if I truly understood before moving on. </li>
  
	  <li><strong>Hands-on Hypothesis Testing:</strong> For code understanding, I read line-by-line, inputting custom values to test my hypotheses about what each function or module should output.</li>
  
	  <li><strong>Refine with Reflection:</strong> Whenever something clicked or didn’t, I noted it down, often returning days later with fresh perspective.</li>
	</ol>

	<!--<h4>Learning Schedule & Experiments</h4>-->
	<h4>Daily Workflow and Experimentation</h4>
        <p>[Fill in: How you set up your learning environment, what experiments you tried, how you structured your daily learning]</p>

	<p> Need to update a bit further: I structured my learning with time-divided approaches: mornings for Andrej's lectures, afternoons for interpretability demo notebooks, and walking time for Chris Olah's interviews on mechanical interpretability perspectives. For complex concepts, I screen-captured code snippets and diagrams, then asked Claude for thorough explanations, leveraging AI tools as learning accelerators rather than replacements for deep understanding.</p>	

      </div>
	

      <div class="subsection">
        <h3>Tools and Resources</h3>
	<p>[This whole section needs to be updated]</p>
        <!--<h4>Primary Learning Resources</h4>-->
	<h4>Core Technical Resources</h4>
        <ul>
	  <li><strong>TransformerLens:</strong> Since Neel Nanda (a leading researcher in mechanistic interpretability) wrote this library, understanding his transformer implementation approach provided valuable insights into both the architecture and the interpretability tooling philosophy.</li>
	  <li><strong>Neel Nanda's Tutorials:</strong> Found his transformer video specifically designed for interpretability research onboarding. Went through his transformer implementation line by line, which took longer than expected but provided solid code-level understanding of building transformers from scratch.</li>
	  <li><strong>Andrej Karpathy's Videos:</strong> Watched his transformer lecture at 2x speed, pausing to write down keywords. Karpathy (former Tesla AI Director, known for exceptional educational content) provided crucial historical "causality" context—how attention mechanisms evolved from previous limitations, including the fascinating detail that attention was first mentioned in earlier papers as "RNN soft search," making query-key-value concepts click.</li>
	  <li><strong>Fundamental Papers:</strong> Need to update: Focused on milestone papers that led to attention mechanisms rather than trying to cover recent publications, prioritizing depth over breadth.</li>
        </ul>

	<h4>Knowledge Processing Tools</h4>
		•	NotebookLM: Used to turn dense source materials into digestible formats (glossary, quiz, audio overview).
	•	Claude, ChatGPT, and Perplexity: Used as thought partners to clarify tough concepts, test understanding, and get alternative explanations for both code and theory.
	For example: When I couldn’t wrap my head around attention weights in a multi-head setting, I would copy diagrams or code snippets into Claude and ask for simplified mental models or analogies. I often used ChatGPT for debugging and sanity-checking my implementation-based hypotheses.
	* Colab, Github

      </div>

      <!-- Section 3: Current Understanding & Physics Parallels -->
      <div class="section-header">
        <h2>3. Current Understanding: Where Physics Meets AI</h2
      </div>

      <div class="subsection">
        <h3>Transformer Architecture Deep Dive</h3>
	<p> In this section, we focus on decoder-only transformers, specifically using the same architecture and parameters as GPT-2 Small.
	  [need to add reference]
	</p>


	<figure class="centered_image">
	  <center>
		<img src="static/transformer/transformer_flowchart_300.drawio.png" width="300px">
	      </center>
	     <figcaption style="text-align: center;">
	    Fig.2: Flowchart of Transformer, Decoder Only, figure slightly modified from xxx
	  </figcaption>
	</figure>
	<br>


	<h4>Foundational Concepts</h4>
        
        <h5>The Residual Stream: Information Highway</h5>
        <p>At the heart of the transformer lies the <strong>residual stream</strong> - the main information highway that carries data through each layer. Unlike traditional neural networks where information gets completely transformed at each step, transformers add modifications to this stream while preserving the original information. This is analogous to how we might annotate a document by adding notes in the margins rather than rewriting the entire text. From a physics perspective, this is similar to perturbation theory - we start with an unperturbed state (the original embeddings) and add small corrections at each layer, with each transformer block acting like a perturbative correction that modifies but doesn't replace the underlying state.</p>

	<h5>Layer Normalization: Preventing Gradient Problems</h5>
	<p>Layer normalization's primary purpose is maintaining stable gradient flow during training. In deep networks, activations can have wildly different scales across layers, causing gradients during backpropagation to become very small (vanishing) or very large (exploding). Layer normalization solves this by normalizing the activations going into each layer, keeping their scale consistent.</p>

	<p>The process: subtract the mean to center at zero, divide by standard deviation to normalize variance, then apply learned scaling and translation parameters. This ensures more stable gradient magnitudes throughout the network during training, allowing the model to learn effectively. The learned scaling (γ) and translation (β) parameters then allow each layer to recover the optimal scale and shift for its specific function, while maintaining the gradient flow benefits.</p>

	<p>This creates more stable optimization dynamics, similar to how physicists normalize variables in numerical simulations to prevent computational instabilities when dealing with vastly different scales.</p>
	

	<h4>The Transformer Flow</h4>
	See Fig.2 above starting from bottom to top

	<h5>1. Tokens: From Text to Integers</h5>
        <p>Everything starts with converting human-readable text into integers through tokenization. This uses a method called Byte-Pair Encoding - we start with individual characters and iteratively merge the most common pairs. The result is a vocabulary of ~50,000 sub-word tokens that can represent any text. Each token gets mapped to a unique integer ID, which serves as an efficient key for the embedding lookup table.</p>
	[As a side note, lookup table is differnet from hash table, key is laready determined using index/ID]

        <h5>2. Embedding: From Integers to Vectors</h5>
        <p>The embedding layer is essentially a giant lookup table that converts each token ID into a high-dimensional vector (768 dimensions in GPT-2). This learned mapping allows the model to represent semantic relationships - similar tokens end up with similar embedding vectors through training.</p>	


        <h5>3. Positional Embeddings: Adding Sequence Information</h5>
        <p>Since attention is content-based rather than position-based, we need to explicitly tell the model where each token sits in the sequence. We learn another lookup table that maps position indices to vectors, then <em>add</em> these to the token embeddings. The key insight that initially confused me: how does adding positional embeddings to token embeddings not create collisons? The answer lies in the high dimensionality - with embedding vectors of length 768, the probability of collision becomes vanishingly small. This is similar to how CRISPR achieved precision in gene editing: while earlier techniques used shorter recognition sequences (higher collision risk), CRISPR uses ~20-nucleotide guide sequences, making the probability of accidentally matching the wrong genomic location negligibly small. </p>

        <h5>4. Attention Mechanism: Intelligent Information Routing</h5>
        <p>This is where the magic happens. For each token, the model computes three vectors: Query (what am I looking for?), Key (what do I contain?), and Value (what information should I pass along?). The similarity between Query and Key vectors determines how much attention each token pays to every other token in the sequence.</p>
	<p>**Note: In decoder-only transformers like GPT-2, this is specifically self-attention - each token attends to other tokens within the same sequence. This differs from encoder-decoder architectures (like the original Transformer) which also include cross-attention, where decoder tokens attend to encoder representations. Since we have no separate encoder, all attention is self-attention.**</p>
	[Add math formular and einstein summation symmetry around the order?]
	
        <p><strong>Multi-Head Attention:</strong> Rather than having one attention mechanism, transformers use multiple "heads" (12 in GPT-2) that attend to different types of information simultaneously. Each head can specialize - one might focus on syntactic relationships, another on semantic meaning. It's like having multiple experts each contributing their specialized perspective.</p>
        

<p><strong>Causal Attention:</strong> In GPT-style models, we use what's called "causal attention" - though I find this term somewhat misleading from a physicist's perspective. While there's a notion of sequence, there's no true temporal causality. Instead, it's a masking mechanism that prevents tokens from <em>seeing</em> later tokens in sequence, maintaining the sequential prediction task. This is analogous to preventing data leakage in ML (where future information accidentally gets into training features) or blinding in experimental physics (where researchers are kept unaware of group assignments to prevent bias). All three techniques share the same core principle: controlling information flow to maintain the integrity of the process.</p>

        <h5>5. MLP Layer: Pattern Recognition and Computation</h5>
	<p>Despite the confusing name "multi-layer perceptron," these are actually simple two-layer feed-forward networks: expand the dimensionality (768 → 3072), apply non-linearity (GELU activation), then contract back (3072 → 768). The expansion (768 → 3072) creates finer granularity by providing more "working space" for complex transformations. The GELU non-linearity then selectively activates different pathways, creating pattern recognition. The contraction (3072 → 768) distills all this rich intermediate computation back into the residual stream format, essentially summarizing the complex high-dimensional processing into a form that can be added back to the information highway. </p>


	<h5>6. Transformer Blocks: The Core Architecture</h5>
	<p>The transformer block is the fundamental building unit, consisting of two key steps that always work together in a specific order:</p>

	<p><strong>1. Attention Mechanism:</strong> Routes and gathers relevant information from across the sequence (Communication Phase) <br>
	  <strong>2. MLP Layer:</strong> Processes and transforms this contextualized information (Computation Phase)</p>

	<p>This order is crucial: attention first allows each position to collect relevant context from the entire sequence, then the MLP can do informed processing with this enriched representation. The alternative (MLP → Attention) would force the MLP to process information blindly at each position before sharing, like trying to analyze the word "bank" without knowing whether the context involves rivers or money. By putting attention first, we follow the principle of "gather information, then process it" rather than "process locally, then share." This makes computation more effective - the MLP gets to do its complex reasoning on contextualized, relevant information rather than wasting computational effort processing isolated local information that may lack important context. </p>

	<p>Each step is preceded by layer normalization (a standard preprocessing necessity before feeding information to any layer, including the final unembed step). Both the attention and MLP outputs are added back to the residual stream via residual connections.</p>

	<p>This transformer block pattern repeats 12 times in GPT-2, with each iteration building increasingly complex representations from the combination of attention-driven information gathering and MLP-driven computation.</p>


	<h5>7. Unembed: Converting to Vocabulary Scores</h5>
	<p>The final step converts the high-dimensional representations into raw scores over the vocabulary. This is conceptually the reverse of embedding - we apply a linear transformation (matrix multiplication plus bias) from the model dimension (768) to vocabulary size (~50,000), creating one logit score for each possible token type in the vocabulary.</p>


	<h5>8. Logits: Raw Predictions</h5>
	<p>These raw scores (shown at the top of Fig. 2) represent the model's unprocessed predictions. During inference, they are converted to probabilities using softmax (not shown in Fig. 2 as it's a mathematical operation, not an architectural component), giving us probability estimates for each possible next token. For token generation, we can sample from this distribution or take the highest probability token. These probabilities also serve as the primary metric for measuring prediction quality, used for cross-entropy loss during both training and evaluation, perplexity calculations, and other language modeling metrics.</p>	

      </div>

      <div class="subsection">
        <h3>Physics Analogies and Insights</h3>
        
        <div class="insight-box">
          <h4>Key Physics Parallels Discovered</h4>
          <p>[Fill in specific analogies you've identified between physics concepts and transformer mechanisms, kinda mentioned in each subsection and can be skipped?]</p>
          
          <h5>Example Areas to Explore:</h5>
          <ul>
	    <li>Superposition: Wave functions (both E&M and QM)</li>
            <li>Symmetries in the attention mechanism</li>
            <li>Energy landscapes and optimization</li>
            <li>Field theory parallels in embeddings</li>
          </ul>
        </div>
      </div>

      <div class="subsection">
        <h3>Interpretability Concepts</h3>
        
        <h4>Circuit Analysis</h4>
        <p>[Fill in: Your understanding of how circuits work in transformers, what you've learned from hands-on exploration]</p>

        <h4>Feature Discovery</h4>
        <p>[Fill in: Superposition problem, sparse features, what you've discovered through TransformerLens]</p>

        <h4>Mechanistic Understanding</h4>
        <p>[Fill in: What "mechanistic interpretability" means to you now, how it differs from other approaches]</p>
      </div>

      <!-- Section 4: Research Questions -->
      <div class="section-header">
        <h2>4. Open Questions: A Physicist's Perspective</h2>
      </div>
      <p>While some of these questions may overlap with existing work, they represent areas where physics intuition suggests different approaches to transformer interpretability—spanning new research directions, methodological considerations, and conceptual foundations.</p>
      
      <div class="subsection">
        <h3>Novel Research Directions</h3>
        <!--<p>[Fill in: Specific research questions you've formulated that leverage your physics background]</p>-->

	<h4>Physics-Inspired Frameworks</h4>
	<p>Could we apply group theory (successfully used to organize the "particle zoo" in the 1950s-60s) to systematically group neural circuit patterns for better interpretability? Just as group theory revealed underlying symmetries that organized seemingly random hadrons into the Eightfold Way and eventually led to the quark model, similar mathematical frameworks might help classify the multitude of neural network pathways and patterns.</p>

	<h4>Many-Body Neural Systems</h4>
	<p>While we understand individual neurons, understanding group behavior remains elusive - reminiscent of the reductionist vs. holistic challenge in physics. Statistical mechanics deals with many-body systems and emergent laws. Could these approaches better explain neural network behavior than trying to understand individual components?</p>
	
	<h4>Scaling Laws as Phase Transitions</h4>
	<p>Do scaling laws in neural networks correspond to phase transitions observed in physics? Certain phenomena require critical mass (like star formation) - what are the analogous thresholds in neural systems where large numbers create qualitatively different behavior?</p>
      </div>

      <div class="subsection">
        <h3>Methodological Questions</h3>
        <!--<p>[Fill in: Questions about experimental design, measurement, analysis approaches that could benefit from physics thinking]</p>-->
	<h4>Measurement Strategy</h4>
	<p>From an experimental physicist's perspective, <em>what</em> we measure is critical. Is loss really the best metric, or should we focus on token probabilities or layer weights? How do we systematically measure emergent behavior with scaling? With so many variables to experiment on, how can we quickly and systematically group or reduce them?</p>

	<h4>Uncertainty vs. Deterministic Classification</h4>
	<p>Physics emphasizes precise measurements with quantified uncertainties, while industry ML often defaults to deterministic outputs. Given that the human brain operates as a Bayesian inference engine—continuously updating beliefs from limited evidence—should interpretability research embrace more explicitly Bayesian frameworks? This could be particularly valuable for understanding how transformers handle infrequent but critical decisions where we can't gather sufficient statistics, similar to how humans make consequential life choices without the luxury of controlled trials.</p>

	<h4>Brain Science vs. Mathematical Approaches</h4>
	<p>Are we tackling the right problem? Is mechanical interpretability the best way to understand transformer decisions, or should we adopt more brain science methodologies? Given that neural networks aim to emulate brains, shouldn't we leverage neuroscience approaches more heavily?</p>

	
      </div>

      <div class="subsection">
        <h3>Conceptual Gaps</h3>
        <!--<p>[Fill in: Areas where you see opportunities for new frameworks or approaches]</p>-->
	<h4>Grounded vs. Abstract Meaning</h4>
	<p>Neural networks feel more mathematical than physical - in physics, information has specific meaning (e.g., particle spin states in wave functions correspond to measurable angular momentum), but in ML, any numerical representation is valid as long as it's useful for the task. Why should certain neuron firing patterns have inherent meaning? This fundamental question about the nature of representation needs addressing.</p>

	<h4>The "Black Box" Problem</h4>
	<p>Why is human understanding of neural network mechanisms considered essential? We accept quantum mechanics despite interpretational unknowns, and historically used many technologies (flight, pharmaceuticals) before understanding mechanisms. However, neural networks may pose unique risks due to their potentially unbounded capabilities and unpredictable failure modes—making this ultimately a policy question about acceptable risk levels.</p>

	<h4>In-Context Learning</h4>
	<p>How does transformer in-context learning compare to human rapid adaptation? The ability to change behavior without parameter updates seems more like pattern recognition than true learning—what can neuroscience tell us about this distinction?</p>
      </div>

      <!-- Section 5: Reflection & Next Steps -->
      <div class="section-header">
        <h2>5. Reflection: What This Process Taught Me</h2>
      </div>

      <div class="subsection">
        <h3>Key Insights</h3>
        <p>[Fill in: Most important things you learned about the field and about your own approach to learning]</p>
      </div>

      <div class="subsection">
        <h3>Confidence in Career Direction</h3>
        <p>[Fill in: How this experience reinforced your decision to pursue interpretability research at Anthropic, need to ponder about it]</p>
      </div>

      <div class="subsection">
        <h3>Next Steps</h3>
        <p>[Fill in: Specific areas you want to explore further, skills to develop, research directions to pursue]</p>
      </div>

      <!-- Appendix -->
      <div class="section-header">
        <h2>Appendix: Resources and Tools</h2>
      </div>

      <div class="row">
        <div class="col-md-6">
          <h4>Papers Read</h4>
          <ul>
            <li>[List papers with brief takeaways]</li>
          </ul>
        </div>
        <div class="col-md-6">
          <h4>Tools Explored</h4>
          <ul>
            <li>[List tools with experience notes]</li>
          </ul>
        </div>
      </div>

      <!-- Footer -->
      <div class="starter-template">
        <hr>
        <p class="text-center">
          <a href="projects.html" class="btn btn-default">← Back to Projects</a>
        </p>
        <p class="text-center">
          Questions about this research? Find me on 
          <a href="https://www.linkedin.com/in/kyungeun-lim" target="_blank">LinkedIn</a> |
          <a href="https://github.com/kyungeunlim" target="_blank">GitHub</a>
        </p>
      </div>

    </div><!-- /.container -->

    <!-- Bootstrap core JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="static/js/bootstrap.min.js"></script>

  </body>
</html>
