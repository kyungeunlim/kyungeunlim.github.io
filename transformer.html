<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="A physicist's systematic exploration of transformer architecture and AI interpretability research">
    <meta name="author" content="Kyungeun Lim">

    <title>Transformer Architecture Deep Dive: A Physics Perspective</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="static/css/bootstrap.css">

    <style>
      .section-header {
        margin-top: 40px;
        margin-bottom: 20px;
        padding-bottom: 10px;
        border-bottom: 2px solid #337ab7;
      }
      .subsection {
        margin-top: 25px;
        margin-bottom: 15px;
      }
      .insight-box {
        background-color: #f8f9fa;
        border-left: 4px solid #337ab7;
        padding: 15px;
        margin: 20px 0;
      }
      .draft-note {
        background-color: #fcf8e3;
        border: 1px solid #faebcc;
        color: #8a6d3b;
        padding: 10px;
        margin-bottom: 20px;
        border-radius: 4px;
      }
      
     .subsection h4 {
         font-size: 20px;
	 color: #555555;  /* Dark grey */
      }
      
      .subsection p, .container p {
        font-size: 16px;
        line-height: 1.6;
      }
      
      .lead {
        font-size: 22px;
        line-height: 1.5;
      }
      
    </style>
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">About</a></li> 
            <li><a href="projects.html">Projects</a></li> 
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

<br><br><br>

    <div class="container">

      <!-- Header -->
      <div class="starter-template">
        <h1>Transformer Architecture Deep Dive</h1>
        <p class="lead">A Physics Perspective on Understanding Neural Attention Mechanisms</p>
        
        <div class="draft-note">
          <strong>Work in Progress:</strong> This is a documentation of my 2-week deep dive into transformer architecture and interpretability research, written as part of my transition to AI safety research.
        </div>
      </div>

      <!-- Executive Summary - To be written last -->
      <div class="section-header">
        <h2>Executive Summary</h2>
      </div>
      <div class="insight-box">
        <p><em>[To be filled after completing the full writeup - will summarize key insights, physics connections discovered, and research questions formulated]</em></p>
      </div>

      <!-- Section 1: Motivation -->
      <div class="section-header">
        <h2>1. Motivation: Why This Deep Dive?</h2>
      </div>

      <div class="subsection">
	<h3>Why I Chose This Specific Research Area</h3>
	<!--<p class="lead">-->
	  <p>As a lifelong learner, I've always been driven by curiosity both broad and deep—from podcasts, audiobooks and videos covering diverse topics to diving into technical tutorials and online courses. However, recent experiences losing close ones and witnessing global upheavals made me acutely aware that time is our most scarce resource. With knowledge and tools exploding around us, I felt an urgent need to be more intentional about where I focus my efforts.</p>

	<p>This led me to reflect deeply: what problems do I feel strongly enough about to dedicate my limited time to? What's the balance between leveraging my existing knowledge and adopting new tools to tackle meaningful challenges?</p>

	<p>The answer traces back to questions that have fascinated me since watching "Ghost in the Shell" in the 90s: What makes a human human? My early conclusion was "memory"—and after learning more about brain science, I'm convinced this intuition was correct. Memory is fundamental to consciousness, behavior, and identity. Recent films like "After Yang" have renewed these questions about the boundary between human and artificial intelligence.</p>

	<p>While I spent years in physics trying to understand our universe, my transition to industry eight years ago was driven by curiosity about the "human world"—though as a physicist, I found it ironic that business operates on empirical rules and psychology rather than fundamental laws.</p>

	<p>Now, with AI systems becoming increasingly sophisticated, I see interpretability research as the perfect intersection of my physics training, my industry experience, and my long-standing fascination with consciousness. Understanding how these systems actually work—reverse-engineering their "memory" and decision-making—feels like the most meaningful problem I can contribute to. It combines the rigor of physics with the profound questions about intelligence that have captivated me for decades.</p>
      </div>
      
      <div class="subsection">
        <h3>The Decision to Commit Two Weeks</h3>
        <p>[Fill in: Why I chose interpretability research, discovering Anthropic's work, deciding to go deep rather than surface-level learning]</p>
      </div>

      <div class="subsection">
        <h3>Anthropic's Interpretability Agenda</h3>
        <p>[Fill in: What drew me to their specific approach - mechanistic interpretability, production model focus, circuit analysis methods]</p>
      </div>

      <!-- Section 2: Learning Methodology -->
      <div class="section-header">
        <h2>2. Learning Methodology: A Physicist's Approach</h2>
      </div>

      <div class="subsection">
        <h3>Prior Knowledge Baseline</h3>
        
        <h4>Neural Networks Foundation</h4>
        <p>I already had basic knowledge of neural networks from my academic time - at the very core, they are inspired by our brain, and their power comes from "activation" functions of neurons, which introduce non-linearity and allow the network to learn complex patterns rather than just linear combinations. I also knew that "deep" means the number of layers is larger, which allows better pattern recognition.</p>

        <h4>Sequential Models Intuition</h4>
        <p>With my physicist mindset, I had this picture in mind: progressively building up the dimensions of information from previous steps. Markov chains utilize one previous step to make predictions, Fibonacci-style problems utilize two previous steps, and time series models utilize multiple previous steps with exponentially decaying weights, focusing more on recent information. RNNs represent the next evolution where we can theoretically use all previous information in a sequence, but they suffer from vanishing gradients over long distances. LSTMs solved this through sophisticated gating mechanisms that can selectively remember and forget information across long sequences. Attention mechanisms represent the latest breakthrough where I don't have to process information linearly—I can intelligently focus on the most relevant parts of the entire sequence simultaneously, making optimal use of multiple previous steps. All of these advances were only possible with the development of powerful computers, allowing many complex operations within much shorter times (similar to how Bayesian inference became practical due to better computing power). Like almost any technological evolution, this represents repeated attempts to solve the same fundamental problem—understanding sequential patterns—by addressing the limitations of previous methodologies with advancing computational capabilities.</p>

        <h4>Previous Transformer Exposure</h4>
        <p>I also skimmed through the "Attention is All You Need" paper years ago and understood that it essentially removed the need for RNNs in many sequence modeling tasks. More recently, I also took Andrew Ng's Coursera courses to have better understanding of both neural networks and deep learning fundamentals as well as generative AI with language models. These were also to gain hands-on implementation experience and build basic knowledge of the necessary tools.</p>

        <h4>Practical Experience</h4>
        <p>In terms of tooling, I built an LSTM-based forecasting model at CloudTrucks which provided good exposure to PyTorch, but I haven't used Transformers previously.</p>
      </div>

      <div class="subsection">
        <h3>Learning Strategy: Fundamentals Over Trends</h3>
        <p>[Fill in: Decision to focus on foundational understanding rather than keeping up with latest papers, rationale for this approach]</p>
      </div>

      <div class="subsection">
        <h3>Tools and Resources</h3>
        
        <h4>Primary Learning Resources</h4>
        <ul>
          <li><strong>TransformerLens:</strong> [Fill in experience with installation and usage]</li>
          <li><strong>Neel Nanda's Tutorials:</strong> [Fill in what you learned from these]</li>
          <li><strong>Andrej Karpathy's Videos:</strong> [Fill in historical context gained]</li>
          <li><strong>Fundamental Papers:</strong> [List specific papers read and takeaways]</li>
        </ul>

        <h4>Experimental Approach</h4>
        <p>[Fill in: How you set up your learning environment, what experiments you tried, how you structured your daily learning]</p>
      </div>

      <!-- Section 3: Current Understanding & Physics Parallels -->
      <div class="section-header">
        <h2>3. Current Understanding: Where Physics Meets AI</h2>
      </div>

      <div class="subsection">
        <h3>Transformer Architecture Deep Dive</h3>
        
        <h4>Attention Mechanism</h4>
        <p>[Fill in: Your understanding of self-attention, how it works mathematically, what makes it powerful]</p>

        <h4>Key Components</h4>
        <ul>
          <li><strong>Multi-Head Attention:</strong> [Your understanding]</li>
          <li><strong>Position Encodings:</strong> [How these work and why needed]</li>
          <li><strong>Feed-Forward Networks:</strong> [Role in the architecture]</li>
          <li><strong>Layer Normalization:</strong> [Purpose and physics parallels]</li>
        </ul>
      </div>

      <div class="subsection">
        <h3>Physics Analogies and Insights</h3>
        
        <div class="insight-box">
          <h4>Key Physics Parallels Discovered</h4>
          <p>[Fill in specific analogies you've identified between physics concepts and transformer mechanisms]</p>
          
          <h5>Example Areas to Explore:</h5>
          <ul>
            <li>Attention weights as probability distributions</li>
            <li>Information flow and conservation principles</li>
            <li>Symmetries in the attention mechanism</li>
            <li>Energy landscapes and optimization</li>
            <li>Field theory parallels in embeddings</li>
          </ul>
        </div>
      </div>

      <div class="subsection">
        <h3>Interpretability Concepts</h3>
        
        <h4>Circuit Analysis</h4>
        <p>[Fill in: Your understanding of how circuits work in transformers, what you've learned from hands-on exploration]</p>

        <h4>Feature Discovery</h4>
        <p>[Fill in: Superposition problem, sparse features, what you've discovered through TransformerLens]</p>

        <h4>Mechanistic Understanding</h4>
        <p>[Fill in: What "mechanistic interpretability" means to you now, how it differs from other approaches]</p>
      </div>

      <!-- Section 4: Research Questions -->
      <div class="section-header">
        <h2>4. Open Questions: A Physicist's Perspective</h2>
      </div>

      <div class="subsection">
        <h3>Novel Research Directions</h3>
        <p>[Fill in: Specific research questions you've formulated that leverage your physics background]</p>
      </div>

      <div class="subsection">
        <h3>Methodological Questions</h3>
        <p>[Fill in: Questions about experimental design, measurement, analysis approaches that could benefit from physics thinking]</p>
      </div>

      <div class="subsection">
        <h3>Conceptual Gaps</h3>
        <p>[Fill in: Areas where you see opportunities for new frameworks or approaches]</p>
      </div>

      <!-- Section 5: Reflection & Next Steps -->
      <div class="section-header">
        <h2>5. Reflection: What This Process Taught Me</h2>
      </div>

      <div class="subsection">
        <h3>Key Insights</h3>
        <p>[Fill in: Most important things you learned about the field and about your own approach to learning]</p>
      </div>

      <div class="subsection">
        <h3>Confidence in Career Direction</h3>
        <p>[Fill in: How this experience reinforced your decision to pursue interpretability research at Anthropic]</p>
      </div>

      <div class="subsection">
        <h3>Next Steps</h3>
        <p>[Fill in: Specific areas you want to explore further, skills to develop, research directions to pursue]</p>
      </div>

      <!-- Appendix -->
      <div class="section-header">
        <h2>Appendix: Resources and Tools</h2>
      </div>

      <div class="row">
        <div class="col-md-6">
          <h4>Papers Read</h4>
          <ul>
            <li>[List papers with brief takeaways]</li>
          </ul>
        </div>
        <div class="col-md-6">
          <h4>Tools Explored</h4>
          <ul>
            <li>[List tools with experience notes]</li>
          </ul>
        </div>
      </div>

      <!-- Footer -->
      <div class="starter-template">
        <hr>
        <p class="text-center">
          <a href="projects.html" class="btn btn-default">← Back to Projects</a>
        </p>
        <p class="text-center">
          Questions about this research? Find me on 
          <a href="https://www.linkedin.com/in/kyungeun-lim" target="_blank">LinkedIn</a> | 
          <a href="https://github.com/kyungeunlim" target="_blank">GitHub</a>
        </p>
      </div>

    </div><!-- /.container -->

    <!-- Bootstrap core JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="static/js/bootstrap.min.js"></script>

  </body>
</html>
