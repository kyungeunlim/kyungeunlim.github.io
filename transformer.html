<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A physicist's systematic exploration of transformer architecture and AI interpretability research">
    <meta name="author" content="Kyungeun Lim">

    <title>From Physics to AI Safety: Cross-Disciplinary Questions for Transformer Interpretability</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="static/css/bootstrap.css">

    <style>
      .section-header {
        margin-top: 40px;
        margin-bottom: 20px;
        padding-bottom: 10px;
        border-bottom: 2px solid #337ab7;
        /*border-bottom: 2px solid #2c5282;*/
      }
      .subsection {
        margin-top: 25px;
        margin-bottom: 15px;
      }
      .container ul li {
	  font-size: 16px;
	  line-height: 1.6;
	  margin-bottom: 8px;
      }
      .insight-box {
        background-color: #f8f9fa;
        border-left: 4px solid #337ab7;
        padding: 15px;
        margin: 20px 0;
      }
      .draft-note {
        background-color: #fcf8e3;
        border: 1px solid #faebcc;
        color: #8a6d3b;
        padding: 10px;
        margin-bottom: 20px;
        border-radius: 4px;
      }

      .subsection h3 {
          font-size: 24px;
	  /*color: #337ab7;*/
	  color: #2c5282;
            margin-bottom: 15px;
        }      
     .subsection h4 {
         font-size: 20px;
	 color: #555555;  /* Dark grey */
      }

      .subsection h5 {
            font-size: 18px;
            color: #666666;
            margin-top: 20px;
            margin-bottom: 8px;
      }
      
      .subsection p, .container p {
        font-size: 16px;
        line-height: 1.6;
        margin-bottom: 12px;	
      }
      
      .lead {
        font-size: 22px;
        line-height: 1.5;
      }
        strong {
            color: #2c5282;
        }
        em {
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 3px;
      }
    </style>
</head>

<body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">About</a></li> 
            <li><a href="projects.html">Projects</a></li> 
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

<br><br><br>

    <div class="container">

<!-- Header -->
<div class="starter-template">
  <h1>From Physics to AI Safety: Cross-Disciplinary Questions for Transformer Interpretability</h1>
  <p class="lead">A two-week deep dive into transformer architecture: a learning journey from fundamentals to physics-inspired research questions.</p>
  
  <!-- Reading Time -->
  <p class="reading-time">
    üìñ <strong>Reading Time:</strong> Executive Summary (2 min) | Full Document (25-30 min)
  </p>
</div>

<!-- Executive Summary -->
<div class="section-header">
  <h2>Executive Summary</h2>
</div>
<p>This document presents a systematic exploration of transformer architecture through the lens of physics training, building foundational knowledge necessary for mechanistic interpretability research while demonstrating how cross-disciplinary approaches can accelerate understanding of complex AI systems. This work illustrates how domain expertise from physics can provide complementary perspectives on AI safety challenges, with research directions identified that could contribute to advancing interpretability methods, understanding scaling phenomena, and developing more rigorous approaches to analyzing AI systems.</p>


      <h3>Key Contributions</h3>
      <ul>
	<li><strong>Physics-Informed Architecture Understanding:</strong> Applied physics frameworks (perturbation theory, many-body systems, statistical mechanics) to understand transformer mechanics, revealing analogies that clarify complex mechanisms like residual streams and attention patterns.</li>
  
	<li><strong>Systematic Learning Methodology:</strong> Developed and documented a transferable 6-step process for efficiently acquiring expertise across technical domains‚Äîvaluable for researchers transitioning into AI safety from other fields.</li>
  
	<li><strong>Physics-Inspired Research Directions:</strong> Identified potential research avenues including group theory applications to neural circuit classification, scaling laws as phase transitions, and statistical mechanics approaches to emergent behavior‚Äîquestions that may offer fresh methodological angles for interpretability research.</li>
  
	<li><strong>Technical Implementation:</strong> Systematically worked through transformer implementations with hands-on exploration of attention mechanisms, testing different inputs and modifications to build deep understanding of architectural components.</li>
      </ul>
      
      <!-- Section 1: Motivation -->
      <div class="section-header">
        <h2>1. Why: Motivation for This Deep Dive</h2>
      </div>

      <div class="subsection">
	<h3>The Pull: Why AI Interpretability Matters</h3>
	<!--<p class="lead">-->
	<p> I've always enjoyed learning broadly and deeply‚Äîfrom podcasts and audiobooks spanning big history, brain science, and philosophy to technical tutorials and online courses‚Äîdriven mostly by curiosity and genuine enjoyment. This habit of connecting insights across seemingly unrelated domains has shaped how I approach complex problems: looking for patterns that emerge when viewing challenges through multiple disciplinary lenses. But this goes beyond personal curiosity‚ÄîI also believe learning isn't optional; it's directly related to our collective survival. The COVID pandemic illustrated this perfectly: without mRNA vaccine technology‚Äîbuilt on decades of accumulated scientific knowledge across multiple fields‚Äîhumanity could have faced an existential threat.</p>

	<p>At the same time, the rapidly exploding pace of new knowledge made me realize I need to learn more efficiently to keep up, while also finding more effective approaches to learning how to learn. Additionally, recently experiencing the loss of loved ones and witnessing devastating events so close to home reinforced that time is our scarcest resource.</p>
	
	 <p>These convergent realizations created a dilemma: I can learn forever and find joy in it, but what's the point if I don't use it for something meaningful? This led me to reflect deeply on what problems I feel strongly enough about to be worth my limited time, and how to balance leveraging my existing knowledge with efficiently adopting new tools to tackle challenges that matter.</p>

	<p>The answer traces back to questions that have fascinated me since watching "Ghost in the Shell" in the 90s: What makes a human human? My early conclusion was "memory"‚Äîand after learning more about brain science, I'm convinced this intuition was correct. Memory is fundamental to emotion, action, identity, and consciousness. Recent films like "After Yang" have renewed these questions about the boundary between human and AI.</p>

	<!--
	<p>This balance became clear when I considered AI interpretability research. The field can benefit from rigorous analytical skills to reverse-engineer complex systems‚Äîdrawing from physics training‚Äîand practical experience bridging theory with real-world applications. Interpretability sits at this intersection, combining both scientific rigor and engineering pragmatism while rapidly evolving with new techniques and tools.</p>-->

	<p>With AI systems becoming increasingly sophisticated, I see interpretability research as the perfect intersection of my physics training, my industry experience, and my long-standing fascination with consciousness. Understanding how these systems actually work‚Äîreverse-engineering their "memory" and decision-making‚Äîfeels like the most meaningful problem I can contribute to. I also believe that understanding current AI is a prerequisite for safely developing artificial superintelligence (ASI). It combines the rigor of physics with the profound questions about intelligence that have captivated me for decades, while addressing what I see as a critical foundation for humanity's AI future.
	</p>
      </div>

      <div class="subsection">
        <h3>This Project: Learning Experiment</h3>
	<p>This project also served as an experiment in applying recent insights from brain science to my learning approach. I'd learned that our brains work differently from muscles‚Äîthe key isn't avoiding fatigue but leveraging how creativity emerges from the default mode network through novel combinations of existing knowledge. I wanted to challenge my traditional "deep focus for a few hours" approach, which I considered my core strength and was often praised for, and become more flexible with planning, revising scope on the fly rather than spending days perfecting initial plans.</p>
	</div>      

      
      <div class="subsection">
        <h3>Planning: Two-Week Timeline & Scope</h3>
	
	<p>With upcoming travel scheduled in four weeks, I saw an opportunity to test whether interpretability research was the right direction for my career transition. Drawing from my previous ultra learning experience, I allocated two weeks for deep exploration, one week for writing, and kept the final week as contingency‚Äîa more realistic timeline than my perfectionist tendencies usually allow.</p>

	<p>The biggest challenge was that the scope would entirely depend on how efficiently I could learn the fundamentals. Based on early conversations with friends familiar with the field, I pivoted from attempting meaningful analysis using existing methods to focusing on building a solid foundational understanding. I reduced the scope to what I could learn effectively and to the unique perspectives or questions I might contribute through a physicist‚Äôs lens. This means I intentionally left out existing interpretability research, which I plan to revisit in future work.</p>


      </div>
      <!--
      <div class="subsection">
        <h3>Anthropic's Interpretability Agenda</h3>
        <p>[Fill in: What drew me to their specific approach - mechanistic interpretability, production model focus, circuit analysis methods, I might skip]</p>
      </div> -->

      <!-- Section 2: Learning Methodology -->
      <div class="section-header">
        <h2>2. How: My Learning Methodology</h2>
      </div>

      <div class="subsection">
        <h3>Prior Knowledge Baseline</h3>
        
        <h4>Neural Networks Foundation</h4>
        <p>I already had basic knowledge of neural networks from my academic time - at the very core, they are inspired by our brain, and their power comes from "activation" functions of neurons, which introduce non-linearity and allow the network to learn complex patterns rather than just linear combinations. I also knew that "deep" means the number of layers is larger, which allows better pattern recognition.</p>

        <h4>Sequential Models Intuition</h4>
	<figure class="centered_image">
	  <center>
	    <img src="static/transformer/sequential_models_evolution.png" width="1000px" >
	  </center>
	  <figcaption>Fig.1: Evolution of Sequential Pattern Understanding: From simple Markov chains to sophisticated attention mechanisms, showing how each approach addresses limitations of previous methods.</figcaption>
	</figure>
	<br>
        <p>With my physicist mindset, I had this picture in mind: progressively building up the dimensions of information from previous steps. Markov chains utilize one previous step to make predictions, Fibonacci-style problems utilize two previous steps, and time series models utilize multiple previous steps with exponentially decaying weights, focusing more on recent information. RNNs represent the next evolution where we can theoretically use all previous information in a sequence, but they suffer from vanishing gradients over long distances. LSTMs solved this through sophisticated gating mechanisms that can selectively remember and forget information across long sequences. Attention mechanisms represent the latest breakthrough where I don't have to process information linearly‚ÄîI can intelligently focus on the most relevant parts of the entire sequence simultaneously, making optimal use of multiple previous steps. All of these advances were only possible with the development of powerful computers, allowing many complex operations within much shorter times (similar to how Bayesian inference became practical due to better computing power). Like almost any technological evolution, this represents repeated attempts to solve the same fundamental problem‚Äîunderstanding sequential patterns‚Äîby addressing the limitations of previous methodologies with advancing computational capabilities.</p>

        <h4>Previous Transformer Exposure</h4>
        <p>I also skimmed through the "Attention is All You Need" paper years ago and understood that it essentially removed the need for RNNs in many sequence modeling tasks. More recently, I also took Andrew Ng's Coursera courses to have a better understanding of both neural networks and deep learning fundamentals as well as generative AI with language models. These were also to gain hands-on implementation experience and build basic knowledge of the necessary tools.</p>

        <h4>Practical Experience</h4>
	<p>I led the development of P-Rex, a personalized recommendation system using XGBoost at CloudTrucks, which gave me hands-on experience with feature engineering and production ML deployment. Later, I built an LSTM-based forecasting model, providing exposure to PyTorch and neural network implementation. My work with PostgreSQL and recommendation systems for search ranking also familiarized me with query-key-value concepts used for attention mechanisms in Transformers, though I hadn't worked directly with Transformer architectures previously.</p>

	
      </div>

      <div class="subsection">
        <h3>Learning Strategy: Fundamentals Over Trends</h3>
	<p>Feeling a strong urgency to understand how AI systems actually work, I discovered Anthropic's Mechanical Interpretability Research Scientist job posting and was immediately drawn to their approach of reverse-engineering neural networks. I wanted to quickly assess whether this was the research direction I should pursue, so I dove into the extensive resources provided in their job posting guidelines. </p>

	<p> However, after initially trying to follow all available sources sequentially, I became overwhelmed. I started pondering whether I needed to explore the most recent research to stay "current" or return to fully understanding fundamentals. Stepping back, I realized that if my ultimate goal was to understand how AI works through reverse-engineering transformers, it made no sense to skip understanding transformers from scratch. This reminded me of spending considerable time on harmonic oscillators in Classical Mechanics and hydrogen atoms in Quantum Mechanics‚Äîfundamentals are essential building blocks.</p>

	<p>Building on that, it's simply impossible to "stay current" in depth without solid basics, and this approach aligned with my physics background and natural preference for understanding foundational principles first. I made a conscious decision to prioritize foundational understanding over keeping up with the latest papers, viewing this as both a reality check on my capacity and a strategic choice that matched my learning style.</p>

	<p>Finally, I wanted to preserve my "novice" perspective‚Äîthose fresh questions and potential challenges to existing assumptions that are valuable precisely because of their outsider viewpoint. I knew this perspective would inevitably disappear as I became more familiar with the field, so I made sure to log these initial insights while they were still genuine. Beginner's perspectives can sometimes reveal overlooked assumptions or alternative approaches that experts might miss, but only if captured before that fresh lens is lost to familiarity.</p>
	


      <div class="subsection">
        <h3>Systematic Learning Approach</h3>
	<p>My approach to learning something fundamentally new‚Äîespecially at the cutting edge‚Äîrelies on structuring the experience to both accelerate early understanding and build deep intuition over time. I've refined this process through years of transitioning between disciplines, and for this deep dive, I leaned heavily on strategies rooted in physics training, modular thinking, brain science insights, and industry experience with rapid iteration cycles. I also specifically sought effective learning resources, and I've distilled this into a repeatable six-step process:</p>

	<h4>1. Learn terminology & connect to existing knowledge</h4>
	<p>Memorize new concepts with clear definitions to create basic building blocks‚ÄîI then only need to figure out hierarchical relationships among them. This follows the Rumpelstiltskin principle: you can't truly understand what you can't name. I also noticed that as I accumulate knowledge across fields, I increasingly spot similar concepts used in different domains.</p>

	<h4>2. Map hierarchical structure</h4>
	<p>In books or papers, this is equivalent to understanding the table of contents. This is where I personally benefit most from LLMs. When deciding whether to read a paper, I ask: 1) summarize the whole paper in a few bullet points, 2) what are the strengths and weaknesses, and 3) how would you improve it? I learned this framework from <a href="https://ko.wikipedia.org/wiki/Î∞ïÌÉúÏõÖ_(1963ÎÖÑ)" target="_blank">Tae-woong Park</a>'s YouTube videos and initially applied it by manually prompting various LLMs. However, as AI tools evolve rapidly, I've since streamlined this process‚ÄîI now delegate these questions directly to <a href="https://notebooklm.google.com/" target="_blank">NotebookLM</a>, which has become my default tool for this type of analysis.</p>

	<h4>3. Skim fast, focus on gaps</h4>
	<p>This is based on the fact that I already have accumulated fundamental knowledge from other work (basic math, science concepts, etc.). To be efficient about learning new things, I can skim through everything at high speed, then focus on parts I don't understand, get stuck on, or find insightful. This approach was inspired by insights from Dr. Moon-ho Park's YouTube review <a href="#ref-park-talentcode" class="citation">[1]</a> of <em>The Talent Code</em> <a href="#ref-talent-code" class="citation">[2]</a>.</p>
	
	<h4>4. Test with hands-on coding</h4>
	<p>This is hypothesis testing‚Äîunderstanding output with different inputs builds real understanding and shows how I can materialize/execute ideas.</p>

	<h4>5. Create visual summaries</h4>
	<p>From my academic experience, I learned that eventually any research result is summarized as either a figure or a table. Diagrams are visual aids I often use to communicate complicated concepts with team members when presenting projects.</p>

	<h4>6. Test through active recall</h4>
	<p>The real test is whether I can explain each concept clearly‚Äîthis is the critical part of turning memory into knowledge. Writing is often a good way to do this, and this blog also serves this purpose.</p>

	<h4> Example: This work</h4>
	<p>A detailed walkthrough of applying this methodology is provided in the <a href="#learning-process-example">Appendix</a>.</p>	
	
      <!-- Section 3: Current Understanding & Physics Parallels -->
      <div class="section-header">
        <h2>3. What: A Physicist‚Äôs Understanding of Transformers</h2>
      </div>

      <div class="subsection">
        <h3>Transformer Architecture Deep Dive</h3>
	  <p>In this section, we focus on a decoder-only transformer, specifically using the same architecture and parameters as GPT-2 Small <a href="#ref-radford2019" class="citation">[3]</a>.</p>


	<figure class="centered_image">
	  <center>
		<img src="static/transformer/transformer_flowchart_300.drawio.png" width="300px">
	      </center>
	     <figcaption style="text-align: center;">
	    Fig.2: Flowchart of the Decoder-Only Transformer. It is slightly modified from the figure shown <a href="https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/Clean_Transformer_Demo_Template.ipynb">here</a>.
	  </figcaption>
	</figure>
	<br>


	<h4>Foundational Concepts</h4>
        
        <h5>The Residual Stream: Information Highway</h5>
        <p>At the heart of the transformer lies the <strong>residual stream</strong> - the main information highway that carries data through each layer. Unlike traditional neural networks where information gets completely transformed at each step, transformers add modifications to this stream while preserving the original information. This is analogous to how we might annotate a document by adding notes in the margins rather than rewriting the entire text. From a physics perspective, this is similar to perturbation theory - we start with an unperturbed state (the original embeddings) and add small corrections at each layer, with each transformer block acting like a perturbative correction that modifies but doesn't replace the underlying state.</p>

	<h5>Layer Normalization: Preventing Gradient Problems</h5>
	<p>Layer normalization's primary purpose is maintaining stable gradient flow during training. In deep networks, activations can have wildly different scales across layers, causing gradients during backpropagation to become very small (vanishing) or very large (exploding). Layer normalization solves this by normalizing the activations going into each layer, keeping their scale consistent.</p>

	<p>The process: subtract the mean to center at zero, divide by standard deviation to normalize variance, then apply learned scaling and translation parameters. This ensures more stable gradient magnitudes throughout the network during training, allowing the model to learn effectively. The learned scaling (Œ≥) and translation (Œ≤) parameters then allow each layer to recover the optimal scale and shift for its specific function, while maintaining the gradient flow benefits.</p>

	<p>This creates more stable optimization dynamics, similar to how physicists normalize variables in numerical simulations to prevent computational instabilities when dealing with vastly different scales.</p>
	

	<h4>The Transformer Flow</h4>
	See Fig.2 above starting from bottom to top.

	<h5>1. Tokens: From Text to Integers</h5>
        <p>Everything starts with converting human-readable text into integers through tokenization. This uses a method called Byte-Pair Encoding - we start with individual characters and iteratively merge the most common pairs. The result is a vocabulary of ~50,000 sub-word tokens that can represent any text. Each token gets mapped to a unique integer ID, which serves as an efficient key for the embedding lookup table.</p>

        <h5>2. Embedding: From Integers to Vectors</h5>
        <p>The embedding layer is essentially a giant lookup table that converts each token ID into a high-dimensional vector (768 dimensions in GPT-2). This learned mapping allows the model to represent semantic relationships - similar tokens end up with similar embedding vectors through training.</p>	


        <h5>3. Positional Embeddings: Adding Sequence Information</h5>
        <p>Since attention is content-based rather than position-based, we need to explicitly tell the model where each token sits in the sequence. We learn another lookup table that maps position indices to vectors, then <em>add</em> these to the token embeddings. The key insight that initially confused me: how does adding positional embeddings to token embeddings not create collisions? The answer lies in the high dimensionality - with embedding vectors of length 768, the probability of collision becomes vanishingly small. This is similar to how CRISPR achieved precision in gene editing: while earlier techniques used shorter recognition sequences (higher collision risk), CRISPR uses ~20-nucleotide guide sequences, making the probability of accidentally matching the wrong genomic location negligibly small. </p>

        <h5>4. Attention Mechanism: Intelligent Information Routing</h5>
        <p>This is where the magic happens. For each token, the model computes three vectors: Query (what am I looking for?), Key (what do I contain?), and Value (what information should I pass along?). The similarity between Query and Key vectors determines how much attention each token pays to every other token in the sequence. (During implementation, I discovered that the order of performing Einstein summation operations within each dot product doesn't affect the results‚Äîin retrospect it makes total sense because of commutativity and associativity, but it was a little fun moment.)</p>
	<p>**Note: In decoder-only transformers like GPT-2, this is specifically self-attention - each token attends to other tokens within the same sequence. This differs from encoder-decoder architectures (like the original Transformer) which also include cross-attention, where decoder tokens attend to encoder representations. Since we have no separate encoder, all attention is self-attention.**</p>
	
        <p><strong>Multi-Head Attention:</strong> Rather than having one attention mechanism, transformers use multiple "heads" (12 in GPT-2) that attend to different types of information simultaneously. Each head can specialize - one might focus on syntactic relationships, another on semantic meaning. It's like having multiple experts each contributing their specialized perspective.</p>

	<p><strong>Causal Attention:</strong> In GPT-style models, we use what's called "causal attention" - though I find this term somewhat misleading from a physicist's perspective. While there's a notion of sequence, there's no true temporal causality. Instead, it's a masking mechanism that prevents tokens from <em>seeing</em> later tokens in sequence, maintaining the sequential prediction task. This is analogous to preventing data leakage in ML (where future information accidentally gets into training features) or blinding in experimental physics (where researchers are kept unaware of group assignments to prevent bias). All three techniques share the same core principle: controlling information flow to maintain the integrity of the process.</p>

	<p><strong>The Complete Attention Process:</strong> The mechanism follows six precise steps: (1) compute Query, Key, and Value vectors by applying learned linear transformations to the input embeddings, (2) calculate raw attention scores by computing dot products between each Query and every Key vector, (3) scale these scores to prevent extremely large values, (4) apply the causal mask to set future positions to negative infinity (preventing information leakage), (5) convert masked scores to probabilities using softmax, and (6) compute the weighted sum of Value vectors using these probabilities, then apply a final linear transformation to produce the output that gets added back to the residual stream.</p>

	
        <h5>5. MLP Layer: Pattern Recognition and Computation</h5>
	<p>Despite the confusing name "multi-layer perceptron," these are actually simple two-layer feed-forward networks: expand the dimensionality (768 ‚Üí 3072), apply non-linearity (GELU activation), then contract back (3072 ‚Üí 768). The expansion (768 ‚Üí 3072) creates finer granularity by providing more "working space" for complex transformations. The GELU non-linearity then selectively activates different pathways, creating pattern recognition. The contraction (3072 ‚Üí 768) distills all this rich intermediate computation back into the residual stream format, essentially summarizing the complex high-dimensional processing into a form that can be added back to the information highway. </p>


	<h5>6. Transformer Blocks: The Core Architecture</h5>
	<p>The transformer block is the fundamental building unit, consisting of two key steps that always work together in a specific order:</p>

	<p><strong>1. Attention Mechanism:</strong> Routes and gathers relevant information from across the sequence (Communication Phase) <br>
	  <strong>2. MLP Layer:</strong> Processes and transforms this contextualized information (Computation Phase)</p>

	<p>This order is crucial: attention first allows each position to collect relevant context from the entire sequence, then the MLP can do informed processing with this enriched representation. The alternative (MLP ‚Üí Attention) would force the MLP to process information blindly at each position before sharing, like trying to analyze the word "bank" without knowing whether the context involves rivers or money. By putting attention first, we follow the principle of "gather information, then process it" rather than "process locally, then share." This makes computation more effective - the MLP gets to do its complex reasoning on contextualized, relevant information rather than wasting computational effort processing isolated local information that may lack important context. </p>

	<p>Each step is preceded by layer normalization (a standard preprocessing necessity before feeding information to any layer, including the final unembed step). Both the attention and MLP outputs are added back to the residual stream via residual connections.</p>

	<p>This transformer block pattern repeats 12 times in GPT-2, with each iteration building increasingly complex representations from the combination of attention-driven information gathering and MLP-driven computation.</p>


	<h5>7. Unembed: Converting to Vocabulary Scores</h5>
	<p>The final step converts the high-dimensional representations into raw scores over the vocabulary. This is conceptually the reverse of embedding - we apply a linear transformation (matrix multiplication plus bias) from the model dimension (768) to vocabulary size (~50,000), creating one logit score for each possible token type in the vocabulary.</p>


	<h5>8. Logits: Raw Predictions</h5>
	<p>These raw scores (shown at the top of Fig. 2) represent the model's unprocessed predictions. During inference, they are converted to probabilities using softmax (not shown in Fig. 2 as it's a mathematical operation, not an architectural component), giving us probability estimates for each possible next token. For token generation, we can sample from this distribution or take the highest probability token. These probabilities also serve as the primary metric for measuring prediction quality, used for cross-entropy loss during both training and evaluation, perplexity calculations, and other language modeling metrics.</p>	

      </div>

      
      <!-- Section 4: Research Questions -->
      <div class="section-header">
        <h2>4. So What?: Physics-Inspired Open Questions</h2>
      </div>
      <p>The following research directions reflect physics-informed perspectives and open questions that I have developed during my exploration; while they may overlap with prior work, I believe they highlight useful angles for further investigation.</p>
      
      <div class="subsection">
        <h3>Potential Research Directions</h3>

	<h4>Group Theory for Neural Circuits</h4>
	<p>Could we apply group theory (successfully used to organize the "particle zoo" in the 1950s-60s) to systematically group neural circuit patterns for better interpretability? Just as group theory revealed underlying symmetries that organized seemingly random hadrons into the Eightfold Way and eventually led to the quark model, similar mathematical frameworks might help classify the multitude of neural network pathways and patterns.</p>

	<h4>Many-Body Neural Systems</h4>
	<p>While we understand individual neurons, understanding group behavior remains elusive - reminiscent of the reductionist vs. holistic challenge in physics. Statistical mechanics deals with many-body systems and macroscopic properties. Could these approaches better explain neural network behavior than trying to understand individual components?</p>

	<h4>Scaling Laws Meet Phase Transitions</h4>
	<p>The sharp, discontinuous emergence of new abilities at critical model scales resembles phase transitions in physics, where smooth parameter changes trigger sudden qualitative shifts. This apparent paradox with smooth scaling laws mirrors how underlying variables (like temperature) change gradually while observable properties (like magnetization) change abruptly at critical points. Could identifying the neural equivalents of critical mass phenomena help predict emergence thresholds, or do we need frameworks that unify both gradual scaling improvements and sharp qualitative transitions in AI systems?</p>
	
      </div>

      <div class="subsection">
        <h3>Methodological Questions</h3>
        <!--<p>[Fill in: Questions about experimental design, measurement, analysis approaches that could benefit from physics thinking]</p>-->
	
	<h4>Measurement-Driven Discovery</h4>
	<p>I've learned that many breakthroughs in brain science emerged from defining precise measurable quantities. Karl Friston's free energy principle exemplifies this pattern‚Äîby mathematically defining free energy as an information-theoretic quantity, he unified previously separate theories under one measurable framework <a href="#ref-friston-fep" class="citation">[4]</a>. Similarly, Geoffrey Hinton's work at University of Toronto applied Boltzmann distributions from statistical mechanics to create breakthrough AI architectures, recently recognized with the 2024 Nobel Prize in Physics <a href="#ref-hinton-nobel" class="citation">[5]</a>.
	  <!--
	Brain science continues advancing through better quantification, especially in consciousness research where <strong>Marcello Massimini's Perturbational Complexity Index (PCI)</strong> achieves 100% accuracy in distinguishing conscious from unconscious states <a href="#ref-massimini-pci" class="citation">[6]</a>, and <strong>Giulio Tononi's Integrated Information Theory</strong> defines Œ¶ (Phi) as a measurable consciousness quantity <a href="#ref-tononi-iit" class="citation">[7]</a>.
	  -->
	  This pattern suggests we should think more systematically about what transformative measurable quantities might be waiting to be defined in AI interpretability research.</p>
	
	<h4>Measurement Strategy</h4>
	<p>Also, from an experimental physicist's perspective, <em>what</em> we measure is critical. While cross-entropy loss is optimized for training, is it actually the best way to understand model behavior? Should we measure raw token probabilities, perplexity, or calibration metrics instead of loss? Beyond probability measures, should we also focus on attention patterns, activation magnitudes, or gradient flows? How do we systematically measure emergent behavior with scaling? With so many measurable variables, how can we efficiently extract the high-level patterns that capture the most meaningful information?</p>

	<h4>Bayesian Approaches to Interpretability</h4>
	<p>Physics emphasizes precise measurements with quantified uncertainties, while industry ML often defaults to deterministic outputs. Given that the human brain operates as a Bayesian inference engine‚Äîcontinuously updating beliefs from limited evidence‚Äîshould interpretability research embrace more explicitly Bayesian frameworks? This could be particularly valuable for understanding how transformers handle infrequent but critical decisions where we can't gather sufficient statistics, similar to how humans make consequential life choices without the luxury of controlled trials.</p>


      </div>

      <div class="subsection">
        <h3>Fundamental Puzzles</h3>
	<h4>Grounded vs. Abstract Meaning</h4>
	<p>Neural networks feel more mathematical than physical - in physics, information has specific meaning (e.g., particle spin states in wave functions correspond to measurable angular momentum), but in ML, any numerical representation is valid as long as it's useful for the task. Does certain neuron firing patterns have inherent meaning? This fundamental question about the nature of representation needs addressing.</p>


	<h4>In-Context Learning</h4>
	<p>How does transformer in-context learning compare to human rapid adaptation? The ability to change behavior without parameter updates seems more like pattern recognition than true learning‚Äîwhat can brain science tell us about this distinction?</p>
      </div>

        <div class="subsection">
          <h3>Thoughts Under Fermentation</h3>
	  <h4>Residual Stream Design Choice</h4>
	  <p>The linear additivity of residual streams makes interpretability possible. This seems to be a good choice, but I wonder if there are any other structural ways to allow interpretability more easily with different design setups.</p>
	</div>
	
      <!-- Section 5: Reflection & Next Steps 
      <div class="section-header">
        <h2>5. Reflection: What This Process Taught Me</h2>
      </div>

      <div class="subsection">
        <h3>Key Insights</h3>
        <p>[Fill in: Most important things you learned about the field and about your own approach to learning]</p>
      </div>

      <div class="subsection">
        <h3>Confidence in Career Direction</h3>
        <p>[Fill in: How this experience reinforced your decision to pursue interpretability research at Anthropic, need to ponder about it]</p>
      </div>

      <div class="subsection">
        <h3>Next Steps</h3>
        <p>[Fill in: Specific areas you want to explore further, skills to develop, research directions to pursue]</p>
      </div>
      -->

      <!-- Appendix -->
      <div class="section-header">
	<h2>Appendix</h2>
	<div class="insight-box">
	  <p>[<em>As of 2025-08-17, this section is incomplete. Needs to be updated.</em>]</p>
	</div>
    
	<!-- Add References as first subsection -->
	<h3>References</h3>
	<ol>
	  <li id="ref-park-talentcode">
	    ÏßÄÏãù Ïù∏ÏÇ¨Ïù¥Îìú. "20ÎåÄÎ°ú ÎèåÏïÑÍ∞ÄÎ©¥ Ïù¥Í≤ÉÎ∂ÄÌÑ∞ ÏùΩÏùÑ Í≤ÅÎãàÎã§." ÎáåÍ≥ºÌïôÏúºÎ°ú ÏûÖÏ¶ùÎêú ÏµúÍ≥†Ïùò Ï±Ö 1Í∂å (Î∞ïÎ¨∏Ìò∏ Î∞ïÏÇ¨ 1Î∂Ä). YouTube. 
	    <a href="https://www.youtube.com/watch?v=79crR-kUGrI" target="_blank">https://www.youtube.com/watch?v=79crR-kUGrI</a>
	  </li>
	  <li id="ref-talent-code">
	    Coyle, Daniel. <em>The Talent Code: Greatness Isn't Born. It's Grown. Here's How.</em> Bantam, 2009.
	  </li>
	  <li id="ref-radford2019">
            Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). 
            <em>Language Models are Unsupervised Multitask Learners</em>. OpenAI. 
            <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">PDF</a>
	  </li>
	  <li id="ref-friston-fep">
	    Friston, K. (2010). The free-energy principle: a unified brain theory? <em>Nature Reviews Neuroscience</em>, 11(2), 127-138.
	  </li>
	  <li id="ref-hinton-nobel">
	    The Nobel Prize in Physics 2024. <em>NobelPrize.org</em>. <a href="https://www.nobelprize.org/prizes/physics/2024/press-release/" target="_blank">Press Release</a>
	  </li>
	  <li id="ref-massimini-pci">
	    Casali, A. G., et al. (2013). A theoretically based index of consciousness independent of sensory processing and behavior. <em>Science Translational Medicine</em>, 5(198), 198ra105.
	  </li>
	  <li id="ref-tononi-iit">
	    Tononi, G. (2008). Integrated information theory. <em>Scholarpedia</em>, 3(3), 4164.
	  </li>
	  
	  <!-- Add other references as needed -->
	</ol>
    
	<h3>Tools and Resources</h3>
	<h3 id="learning-process-example">Example: Applying My Six-Step Learning Process</h3>

	<!-- Footer -->
	<div class="starter-template">
	  <hr>
	  <p class="text-center">
	    <a href="projects.html" class="btn btn-default">‚Üê Back to Projects</a>
	  </p>
	  <p class="text-center">
	    Questions about this research? Find me on
	    <a href="https://www.linkedin.com/in/kyungeun-lim" target="_blank">LinkedIn</a> |
	    <a href="https://github.com/kyungeunlim" target="_blank">GitHub</a>
	  </p>
	</div>
      
    </div><!-- /.container -->

    <!-- Bootstrap core JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="static/js/bootstrap.min.js"></script>

  </body>
</html>
